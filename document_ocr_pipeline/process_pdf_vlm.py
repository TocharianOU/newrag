#!/usr/bin/env python3
"""
PDF æ–‡æ¡£æ™ºèƒ½ OCR å¤„ç†è„šæœ¬ï¼ˆæ”¯æŒ VLM ä¿®æ­£ï¼‰
åœ¨åŸæœ‰ adaptive_ocr_pipeline.py åŸºç¡€ä¸Šå¢åŠ  VLM æ™ºèƒ½ä¿®æ­£
"""

import argparse
import json
import sys
import subprocess
from pathlib import Path
from typing import Dict, Any, Tuple
import structlog
import pdfplumber

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config import config

# åˆå§‹åŒ–æ—¥å¿—
logger = structlog.get_logger(__name__)

# å°è¯•å¯¼å…¥ VLM
HAS_VLM = False
try:
    from src.models import VisionModel
    HAS_VLM = True
except Exception as e:
    logger.warning(f"VLM not available: {e}")


def should_use_vlm_refinement(ocr_data: Dict[str, Any]) -> Tuple[bool, str, Dict[str, Any]]:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦ VLM ä¿®æ­£
    
    Args:
        ocr_data: OCR åŸå§‹ç»“æœ
    
    Returns:
        (æ˜¯å¦éœ€è¦ä¿®æ­£, åŸå› , ç»Ÿè®¡ä¿¡æ¯)
    """
    text_blocks = ocr_data.get('text_blocks', [])
    
    if not text_blocks:
        return False, "æ— æ–‡æœ¬å†…å®¹", {}
    
    # ç»Ÿè®¡åˆ†æ
    confidences = [b.get('confidence', 0) for b in text_blocks if b.get('confidence', 0) > 0]
    avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
    
    # æ£€æµ‹ä¹±ç å­—ç¬¦
    all_text = ' '.join([b.get('text', '') for b in text_blocks])
    garbled_chars = sum(1 for c in all_text if ord(c) > 0x4E00 and c in 'ï¿½â–¡â–ªï¸â—†â– â—â—‹â—‡')
    garbled_ratio = garbled_chars / len(all_text) if all_text else 0.0
    
    # æ£€æµ‹ URL ç‰‡æ®µï¼ˆå¯èƒ½è¯†åˆ«é”™è¯¯ï¼‰
    has_url_fragments = any(pattern in all_text.lower() for pattern in ['http', 'www.', '.com', '://', 'https'])
    
    # æ£€æµ‹æ–‡ä»¶åˆ—è¡¨æ¨¡å¼
    lines = [b.get('text', '').strip() for b in text_blocks if b.get('text', '').strip()]
    short_lines = sum(1 for line in lines if len(line) < 50)
    is_file_list = (
        short_lines > 5 and 
        short_lines / len(lines) > 0.6 if lines else False
    ) or any(ext in all_text.lower() for ext in ['.tar', '.dmg', '.pkg', '.pdf', '.docx'])
    
    # æ£€æµ‹å¤šè¡ŒçŸ­æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨/ç›®å½•ï¼‰
    is_multi_short_lines = len(lines) >= 5 and short_lines / len(lines) > 0.7 if lines else False
    
    # æ£€æµ‹æ€ç»´å¯¼å›¾/å…³ç³»å›¾ï¼ˆæ ‘å½¢ç¬¦å·å¯†åº¦ï¼‰
    tree_symbols = sum(all_text.count(s) for s in ['â”œ', 'â””', 'â”‚', 'â”€â”€', 'â”€'])
    arrow_symbols = sum(all_text.count(s) for s in ['â†’', 'â†', 'â†“', 'â†‘', 'â‡’', 'â‡', 'â–¶', 'â—€'])
    is_mindmap = (tree_symbols > 5 or arrow_symbols > 3) and len(text_blocks) > 8
    
    stats = {
        'avg_confidence': avg_confidence,
        'garbled_ratio': garbled_ratio,
        'is_file_list': is_file_list,
        'is_multi_short_lines': is_multi_short_lines,
        'has_url_fragments': has_url_fragments,
        'is_mindmap': is_mindmap,
        'tree_symbols_count': tree_symbols,
        'arrow_symbols_count': arrow_symbols,
        'total_blocks': len(text_blocks),
        'total_chars': len(all_text)
    }
    
    # å®½æ¾ä»‹å…¥ç­–ç•¥
    if avg_confidence < 0.8:  # 80% ä»¥ä¸‹å°±ä¿®æ­£
        return True, f"è¯†åˆ«è´¨é‡å¯æå‡ (ç½®ä¿¡åº¦ {avg_confidence:.1%})", stats
    elif garbled_ratio > 0.005:  # 0.5% ä¹±ç å³è§¦å‘
        return True, f"æ£€æµ‹åˆ°ä¹±ç  ({garbled_ratio:.1%})", stats
    elif stats.get('is_mindmap', False):  # æ€ç»´å¯¼å›¾
        return True, "æ£€æµ‹åˆ°æ€ç»´å¯¼å›¾/å…³ç³»å›¾", stats
    elif has_url_fragments:  # URL å¯èƒ½è¯†åˆ«é”™è¯¯
        return True, "æ£€æµ‹åˆ° URLï¼Œéœ€è¦ä¿®æ­£", stats
    elif is_file_list or is_multi_short_lines:  # ç‰¹æ®Šæ ¼å¼
        return True, "æ£€æµ‹åˆ°åˆ—è¡¨ç»“æ„", stats
    
    return False, "è´¨é‡è‰¯å¥½", stats


def refine_text_with_vlm(
    image_path: Path,
    ocr_text: str,
    vlm_model,
    confidence_info: Dict[str, Any] = None
) -> str:
    """
    ä½¿ç”¨ VLM ä¿®æ­£ OCR æ–‡æœ¬
    
    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        ocr_text: OCR åŸå§‹æ–‡æœ¬
        vlm_model: VisionModel å®ä¾‹
        confidence_info: ç½®ä¿¡åº¦ä¿¡æ¯
    
    Returns:
        ä¿®æ­£åçš„æ–‡æœ¬
    """
    if not HAS_VLM or not vlm_model:
        return ocr_text
    
    try:
        # æ„å»ºè´¨é‡æç¤ºä¿¡æ¯
        quality_note = ""
        context_hint = ""
        correction_level = ""
        
        if confidence_info:
            avg_conf = confidence_info.get('avg_confidence', 0)
            garbled_ratio = confidence_info.get('garbled_ratio', 0)
            has_url = confidence_info.get('has_url_fragments', False)
            is_file_list = confidence_info.get('is_file_list', False)
            
            if avg_conf < 0.5:
                quality_note = f"\næ³¨æ„ï¼šOCR è¯†åˆ«è´¨é‡è¾ƒä½ï¼ˆå¹³å‡ç½®ä¿¡åº¦ {avg_conf:.1%}ï¼‰ï¼Œå¯èƒ½å­˜åœ¨è¾ƒå¤šé”™è¯¯ã€‚"
                correction_level = "ã€æ¿€è¿›ä¿®æ­£æ¨¡å¼ã€‘è¯†åˆ«è´¨é‡å¾ˆä½ï¼Œéœ€è¦å¤§å¹…ä¿®æ­£é”™åˆ«å­—å’Œç»“æ„"
            elif avg_conf < 0.7:
                quality_note = f"\næ³¨æ„ï¼šOCR è¯†åˆ«è´¨é‡ä¸­ç­‰ï¼ˆå¹³å‡ç½®ä¿¡åº¦ {avg_conf:.1%}ï¼‰ï¼Œå¯èƒ½æœ‰é”™è¯¯ã€‚"
                correction_level = "ã€ä¸­ç­‰ä¿®æ­£æ¨¡å¼ã€‘é€‚åº¦ä¿®æ­£æ˜æ˜¾çš„é”™åˆ«å­—ï¼Œä¿ç•™å¤§éƒ¨åˆ†åŸæ–‡"
            elif avg_conf < 0.8:
                quality_note = f"\næ³¨æ„ï¼šOCR è¯†åˆ«è´¨é‡å°šå¯ï¼ˆå¹³å‡ç½®ä¿¡åº¦ {avg_conf:.1%}ï¼‰ã€‚"
                correction_level = "ã€ä¿å®ˆä¿®æ­£æ¨¡å¼ã€‘ä»…ä¿®æ­£æ˜æ˜¾é”™è¯¯ï¼Œä¿ç•™æ ¼å¼å’Œè¾¹è·"
            
            if garbled_ratio > 0.005:
                quality_note += f"\næ³¨æ„ï¼šæ£€æµ‹åˆ° {garbled_ratio:.1%} çš„ä¹±ç å­—ç¬¦ï¼Œè¯·å‚è€ƒå›¾ç‰‡ä¿®æ­£ã€‚"
            if has_url:
                context_hint = "è¿™æ˜¯åŒ…å« URL é“¾æ¥çš„å†…å®¹ï¼Œè¯·ç¡®ä¿ URL æ ¼å¼æ­£ç¡®"
            elif is_file_list:
                context_hint = "è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶åˆ—è¡¨/ç›®å½•"
        
        prompt = f"""è¯·æ ¹æ®å›¾ç‰‡å’Œ OCR è¯†åˆ«ç»“æœï¼Œä¿®æ­£ä»¥ä¸‹æ–‡æœ¬ä¸­çš„é”™è¯¯ï¼š

OCR åŸå§‹ç»“æœï¼š
{ocr_text}

è¯†åˆ«è´¨é‡ä¿¡æ¯ï¼š
{quality_note}
{correction_level}

ä¿®æ­£è¦æ±‚ï¼š
1. **é”™åˆ«å­—ä¿®æ­£**ï¼ˆå¿…é¡»å‚è€ƒå›¾ç‰‡ï¼‰ï¼š
   - å®¹å™¨ç›‘æ§/åº”ç”¨ç›‘æ§/æ•°æ®åº“ç›‘æ§ ç­‰ITæœ¯è¯­
   - å¸¸è§é”™è¯¯ï¼šå®¢å™¨â†’å®¹å™¨ã€ç”³é—´â†’ç©ºé—´ã€Vå¿—â†’æ—¥å¿—ã€ç¦ºâ†’åŸŸ
   - ä¸“æœ‰åè¯ï¼šCyberArkã€Kongã€API Gatewayã€CMDB
   - URL é“¾æ¥æ ¼å¼ï¼šhttp/https, ://, åŸŸå

2. **æ ¼å¼ä¿ç•™**ï¼ˆç¦æ­¢ä¿®æ”¹ï¼‰ï¼š
   - æ ‘å½¢ç¬¦å·ï¼šâ”œ â”‚ â”” â”€â”€ 
   - ç¼©è¿›å±‚çº§ï¼šå¿…é¡»ä¸åŸæ–‡ä¸€è‡´
   - æ¢è¡Œä½ç½®ï¼šä¿æŒåŸæœ‰å¸ƒå±€

3. **ç»“æ„ä¿®å¤**ï¼š
   - è¡¥å……ä¸¢å¤±çš„ç¬¦å·ï¼ˆ/, -, |, â”œ, â””ï¼‰
   - æ¢å¤æ–‡ä»¶/æ–‡ä»¶å¤¹å±‚çº§å…³ç³»
   - ä¿®æ­£ URL æ–­è¡Œ/ç©ºæ ¼é”™è¯¯
   - åˆå¹¶è¢«é”™è¯¯åˆ†å‰²çš„è¯è¯­

4. **ç¦æ­¢è¡Œä¸º**ï¼š
   - ä¸è¦æ·»åŠ åŸå›¾ä¸­æ²¡æœ‰çš„å†…å®¹
   - ä¸è¦æ”¹å˜æŠ€æœ¯æœ¯è¯­çš„å«ä¹‰
   - ä¸è¦åˆ é™¤çœ‹ä¼¼é‡å¤ä½†å®é™…å­˜åœ¨çš„å†…å®¹

{f'æç¤ºï¼š{context_hint}' if context_hint else ''}

è¯·ç›´æ¥è¿”å›ä¿®æ­£åçš„æ–‡æœ¬å†…å®¹ï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šã€‚"""

        logger.info("ğŸ¤– è°ƒç”¨ VLM ä¿®æ­£æ–‡æœ¬...",
                   image=str(image_path.name),
                   ocr_length=len(ocr_text),
                   avg_confidence=confidence_info.get('avg_confidence', 0) if confidence_info else 0)
        
        response = vlm_model.extract_text_from_image(str(image_path), prompt)
        refined_text = response.get('text', ocr_text)
        
        # åŸºæœ¬éªŒè¯ï¼šé˜²æ­¢ VLM å¹»è§‰æˆ–æˆªæ–­
        if len(refined_text) < len(ocr_text) * 0.3 or len(refined_text) > len(ocr_text) * 5:
            logger.warning("âš ï¸  VLM ä¿®æ­£ç»“æœé•¿åº¦å¼‚å¸¸ï¼Œä½¿ç”¨åŸå§‹ OCR",
                          original_len=len(ocr_text),
                          refined_len=len(refined_text))
            return ocr_text
        
        logger.info("âœ… VLM ä¿®æ­£å®Œæˆ",
                   original_len=len(ocr_text),
                   refined_len=len(refined_text),
                   change_ratio=f"{(len(refined_text)/len(ocr_text)-1)*100:+.1f}%")
        
        return refined_text
        
    except Exception as e:
        logger.error(f"âŒ VLM ä¿®æ­£å¤±è´¥: {e}", image_path=str(image_path))
        return ocr_text


def process_pdf_page_with_vlm(
    page_image_path: Path,
    ocr_json_path: Path,
    output_dir: Path,
    vlm_model = None
) -> Dict[str, Any]:
    """
    å¯¹å•ä¸ª PDF é¡µé¢çš„ OCR ç»“æœåº”ç”¨ VLM ä¿®æ­£
    
    Args:
        page_image_path: é¡µé¢å›¾ç‰‡è·¯å¾„
        ocr_json_path: OCR JSON ç»“æœè·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        vlm_model: VisionModel å®ä¾‹
    
    Returns:
        ä¿®æ­£ç»“æœ
    """
    # è¯»å– OCR ç»“æœ
    with open(ocr_json_path, 'r', encoding='utf-8') as f:
        ocr_data = json.load(f)
    
    text_blocks = ocr_data.get('text_blocks', [])
    original_text = ocr_data.get('text', '')
    if not original_text and text_blocks:
        original_text = '\n'.join([b.get('text', '') for b in text_blocks if b.get('text')])
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦ VLM ä¿®æ­£
    need_vlm, reason, stats = should_use_vlm_refinement(ocr_data)
    
    logger.info(f"  ğŸ¯ è´¨é‡åˆ†æ:", **stats)
    logger.info(f"  {'âœ…' if need_vlm else 'âŒ'} VLM ä¿®æ­£: {reason}")
    
    final_text = original_text
    vlm_refined = False
    
    if need_vlm and HAS_VLM and vlm_model:
        confidence_info = {
            'avg_confidence': stats['avg_confidence'],
            'garbled_ratio': stats['garbled_ratio'],
            'has_url_fragments': stats.get('has_url_fragments', False),
            'is_file_list': stats.get('is_file_list', False)
        }
        
        final_text = refine_text_with_vlm(
            image_path=page_image_path,
            ocr_text=original_text,
            vlm_model=vlm_model,
            confidence_info=confidence_info
        )
        
        if final_text != original_text:
            vlm_refined = True
            logger.info("  âœ… VLM ä¿®æ­£å®Œæˆ",
                       original_len=len(original_text),
                       refined_len=len(final_text))
    
    return {
        'text': final_text,
        'vlm_refined': vlm_refined,
        'stats': stats,
        'reason': reason
    }


def main():
    parser = argparse.ArgumentParser(description='PDF æ–‡æ¡£æ™ºèƒ½ OCR å¤„ç†ï¼ˆæ”¯æŒ VLM ä¿®æ­£ï¼‰')
    parser.add_argument('pdf_path', type=str, help='PDF æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ocr-engine', type=str, default='vision',
                       choices=['vision', 'paddle', 'easy'],
                       help='OCR å¼•æ“é€‰æ‹© (é»˜è®¤: vision)')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šPDFå_adaptiveï¼‰')
    
    args = parser.parse_args()
    
    pdf_path = Path(args.pdf_path)
    if not pdf_path.exists():
        print(f"âŒ PDF ä¸å­˜åœ¨: {pdf_path}")
        sys.exit(1)
    
    # å®‰å…¨å¤„ç†ä¸­æ–‡æ–‡ä»¶åï¼šå¦‚æœåŒ…å«é ASCII å­—ç¬¦ï¼Œå…ˆå¤åˆ¶ä¸ºä¸´æ—¶æ–‡ä»¶
    temp_process_file = None
    try:
        original_pdf_path = pdf_path
        if any(ord(c) > 127 for c in str(pdf_path)):
            import shutil
            import uuid
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_name = f"temp_process_{uuid.uuid4().hex}.pdf"
            temp_process_file = pdf_path.parent / temp_name
            shutil.copy2(pdf_path, temp_process_file)
            logger.info(f"  ğŸ”„ ä½¿ç”¨å®‰å…¨ä¸´æ—¶æ–‡ä»¶å¤„ç†ä¸­æ–‡åæ–‡ä»¶: {pdf_path.name} -> {temp_name}")
            pdf_path = temp_process_file
        
        # ç¡®å®šè¾“å‡ºç›®å½•ï¼ˆä¸ adaptive_ocr_pipeline.py ä¿æŒä¸€è‡´ï¼‰
        if args.output_dir:
            output_dir = Path(args.output_dir)
        else:
            # ä½¿ç”¨ä¸ adaptive_ocr_pipeline.py ç›¸åŒçš„å‘½åè§„åˆ™
            # æ³¨æ„ï¼šè¿™é‡Œä»ä½¿ç”¨åŸå§‹æ–‡ä»¶åç”Ÿæˆç›®å½•åï¼Œå› ä¸ºç›®å½•åæ”¯æŒä¸­æ–‡
            output_dir = Path(original_pdf_path.stem.replace(' ', '_') + "_adaptive")
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("=" * 80)
        logger.info("ğŸ“„ å¼€å§‹å¤„ç† PDF æ–‡æ¡£ï¼ˆæ™ºèƒ½ VLM æ¨¡å¼ï¼‰", pdf=original_pdf_path.name, ocr_engine=args.ocr_engine)
        logger.info("=" * 80)
        
        # åˆå§‹åŒ– VLM
        vlm_model = None
        if HAS_VLM:
            try:
                vlm_config = config.vision_config
                if vlm_config.get('enabled', False):
                    vlm_model = VisionModel(vlm_config)
                    logger.info("âœ… VLM å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"âš ï¸  VLM åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # å…ˆè°ƒç”¨åŸæœ‰çš„ adaptive_ocr_pipeline ç”ŸæˆåŸºç¡€ OCR
        logger.info("ğŸ“ é˜¶æ®µ 1: è¿è¡Œ Adaptive OCR Pipeline...")
        
        # è°ƒç”¨ adaptive_ocr_pipeline.py ä½œä¸ºå­è¿›ç¨‹
        import subprocess
        adaptive_script = Path('document_ocr_pipeline/adaptive_ocr_pipeline.py')
        subprocess.run([
            sys.executable,
            str(adaptive_script),
            str(pdf_path),
            '--ocr-engine', args.ocr_engine,
            '--output-dir', str(output_dir)
        ], check=True, cwd=project_root)
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_process_file and temp_process_file.exists():
            try:
                temp_process_file.unlink()
                logger.info(f"  ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_process_file.name}")
            except Exception as e:
                logger.warning(f"  âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    # è¯»å–ç”Ÿæˆçš„ complete_adaptive_ocr.json
    complete_json = output_dir / "complete_adaptive_ocr.json"
    if not complete_json.exists():
        logger.error("âŒ Adaptive OCR è¾“å‡ºæœªæ‰¾åˆ°")
        sys.exit(1)
    
    with open(complete_json, 'r', encoding='utf-8') as f:
        adaptive_data = json.load(f)
    
    logger.info("ğŸ“ é˜¶æ®µ 2: VLM æ™ºèƒ½ä¿®æ­£...")
    
    # å¯¹æ¯ä¸€é¡µåº”ç”¨ VLM ä¿®æ­£
    pages = adaptive_data.get('pages', [])
    pages_for_index = []
    
    for page in pages:
        page_num = page.get('page_number')
        logger.info(f"  ğŸ“„ å¤„ç†ç¬¬ {page_num} é¡µ...")
        
        # è·å–é¡µé¢å›¾ç‰‡å’Œ OCR JSON
        stage1 = page.get('stage1_global', {})
        image_filename = stage1.get('image', f'page_{page_num:03d}_300dpi.png')
        ocr_json_filename = stage1.get('ocr_json', f'page_{page_num:03d}_global_ocr.json')
        
        page_image_path = output_dir / image_filename
        ocr_json_path = output_dir / ocr_json_filename
        
        if not page_image_path.exists() or not ocr_json_path.exists():
            logger.warning(f"  âš ï¸  é¡µé¢æ–‡ä»¶ç¼ºå¤±ï¼Œè·³è¿‡")
            continue
        
        # VLM ä¿®æ­£
        vlm_result = process_pdf_page_with_vlm(
            page_image_path=page_image_path,
            ocr_json_path=ocr_json_path,
            output_dir=output_dir,
            vlm_model=vlm_model
        )
        
        # æ›´æ–°é¡µé¢ç»Ÿè®¡ä¿¡æ¯
        if 'statistics' not in page:
            page['statistics'] = {}
        
        page['statistics']['avg_ocr_confidence'] = vlm_result['stats']['avg_confidence']
        page['statistics']['vlm_refined'] = vlm_result['vlm_refined']
        
        # æ„å»ºç´¢å¼•æ–‡æ¡£ï¼ˆæ ‡å‡†æ ¼å¼ï¼Œå…¼å®¹ document_processor.pyï¼‰
        pages_for_index.append({
            'page_number': page_num,
            'image_path': str(page_image_path),
            'image_filename': image_filename,
            'content': {
                'full_text_cleaned': vlm_result['text'],
                'full_text_raw': vlm_result['text'],
                'key_fields': [],
                'tables': []
            },
            'ocr_data': {
                'text_blocks': []  # å¯åç»­è¡¥å……
            },
            'metadata': {
                'extraction_method': 'ocr_vlm_refined' if vlm_result['vlm_refined'] else 'ocr',
                'ocr_engine': args.ocr_engine,
                'avg_ocr_confidence': vlm_result['stats']['avg_confidence'],
                'vlm_refined': vlm_result['vlm_refined']
            }
        })
    
    # ä¿å­˜æ›´æ–°åçš„ complete_adaptive_ocr.json
    with open(complete_json, 'w', encoding='utf-8') as f:
        json.dump(adaptive_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜å¯æœç´¢æ–‡æœ¬ï¼ˆç”¨äº ES ç´¢å¼•ï¼‰
    complete_document_path = output_dir / "complete_document.json"
    with open(complete_document_path, 'w', encoding='utf-8') as f:
        json.dump({'pages': pages_for_index}, f, ensure_ascii=False, indent=2)
    
    logger.info("=" * 80)
    logger.info("ğŸ‰ PDF å¤„ç†å®Œæˆ!")
    logger.info(f"  ğŸ“Š æ€»é¡µæ•°: {len(pages)}")
    vlm_count = sum(1 for p in pages_for_index if p.get('extraction_method') == 'ocr_vlm_refined')
    logger.info(f"  ğŸ¤– VLM ä¿®æ­£: {vlm_count}/{len(pages)} é¡µ")
    logger.info("=" * 80)


if __name__ == '__main__':
    main()


#!/usr/bin/env python3
"""
PPTX å®Œæ•´å¤„ç†ç®¡é“
ç”Ÿæˆä¸ PDF æµç¨‹ä¸€è‡´çš„è¾“å‡ºç»“æ„
"""

import sys
import json
import argparse
from pathlib import Path
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE
import io
from PIL import Image
import subprocess

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from document_ocr_pipeline.extract_document import DocumentExtractor
from document_ocr_pipeline.visualize_extraction import visualize_extraction


def extract_slide_content(slide, slide_num, output_dir, ocr_engine='paddle'):
    """
    æå–å•é¡µå†…å®¹ï¼šæ–‡æœ¬ + å›¾ç‰‡OCR + VLMç»„åˆ
    """
    slide_data = {
        "page_number": slide_num,
        "statistics": {},
        "stage1_global": {},
        "stage3_vlm": {}
    }
    
    # ==================== é˜¶æ®µ 1: æ–‡æœ¬ç›´æ¥æå– ====================
    print(f"\n  ğŸ“ é˜¶æ®µ1: æå–æ–‡æœ¬å†…å®¹ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰...")
    
    extracted_text = {
        "title": "",
        "body": [],
        "notes": "",
        "tables": []
    }
    
    for shape in slide.shapes:
        if shape.has_text_frame:
            text = shape.text.strip()
            if text:
                if hasattr(shape, "is_placeholder") and shape.is_placeholder:
                    placeholder = shape.placeholder_format
                    if placeholder.type == 1:  # Title
                        extracted_text["title"] = text
                    else:
                        extracted_text["body"].append(text)
                else:
                    extracted_text["body"].append(text)
        
        if shape.has_table:
            table = shape.table
            table_data = []
            for row in table.rows:
                row_data = [cell.text for cell in row.cells]
                table_data.append(row_data)
            extracted_text["tables"].append(table_data)
    
    if slide.has_notes_slide:
        extracted_text["notes"] = slide.notes_slide.notes_text_frame.text
    
    # ä¿å­˜æ–‡æœ¬æå–ç»“æœ
    text_json_path = output_dir / f"page_{slide_num:03d}_extracted_text.json"
    with open(text_json_path, 'w', encoding='utf-8') as f:
        json.dump(extracted_text, f, ensure_ascii=False, indent=2)
    
    print(f"    âœ“ æ ‡é¢˜: {extracted_text['title'][:50] if extracted_text['title'] else 'æ— '}")
    print(f"    âœ“ æ–‡æœ¬æ®µè½: {len(extracted_text['body'])} ä¸ª")
    print(f"    âœ“ è¡¨æ ¼: {len(extracted_text['tables'])} ä¸ª")
    
    # ==================== é˜¶æ®µ 2: å¤„ç†å›¾ç‰‡ï¼ˆOCRï¼‰ ====================
    print(f"  ğŸ–¼ï¸  é˜¶æ®µ2: å¤„ç†å›¾ç‰‡å†…å®¹...")
    
    images = []
    image_ocr_results = []
    
    for idx, shape in enumerate(slide.shapes, 1):
        if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
            image = shape.image
            image_filename = f"page_{slide_num:03d}_img_{idx}.{image.ext}"
            image_path = output_dir / image_filename
            
            # ä¿å­˜å›¾ç‰‡
            with open(image_path, "wb") as f:
                f.write(image.blob)
            
            with Image.open(io.BytesIO(image.blob)) as img:
                width, height = img.size
            
            images.append({
                "id": idx,
                "path": str(image_path),
                "format": image.ext,
                "size": [width, height]
            })
            
            print(f"    âœ“ å›¾ç‰‡ {idx}: {width}x{height} ({image.ext})")
            
            # å¯¹å›¾ç‰‡è¿è¡Œ OCR
            ocr_json_path = output_dir / f"page_{slide_num:03d}_img_{idx}_ocr.json"
            try:
                extractor = DocumentExtractor(ocr_engine=ocr_engine)
                ocr_result = extractor.extract_from_image(str(image_path))
                
                with open(ocr_json_path, 'w', encoding='utf-8') as f:
                    json.dump(ocr_result, f, ensure_ascii=False, indent=2)
                
                # ç”Ÿæˆå¯è§†åŒ–
                vis_path = output_dir / f"page_{slide_num:03d}_img_{idx}_visualized.png"
                visualize_extraction(str(image_path), str(ocr_json_path), str(vis_path))
                
                image_ocr_results.append({
                    "image_id": idx,
                    "ocr_json": str(ocr_json_path),
                    "visualized": str(vis_path),
                    "text_blocks_count": len(ocr_result.get('text_blocks', []))
                })
                
                print(f"      âœ“ OCR: {len(ocr_result.get('text_blocks', []))} ä¸ªæ–‡æœ¬å—")
            except Exception as e:
                print(f"      âœ— OCRå¤±è´¥: {e}")
    
    # ==================== é˜¶æ®µ 3: VLM å¤„ç†ï¼ˆå¸¦æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼‰ ====================
    print(f"  ğŸ¤– é˜¶æ®µ3: VLM ç»¼åˆåˆ†æ...")
    
    # æ„å»º VLM æç¤ºï¼ˆåŒ…å«å·²æå–çš„æ–‡æœ¬ï¼‰
    vlm_context = {
        "extracted_text": extracted_text,
        "images": images,
        "image_ocr_results": image_ocr_results
    }
    
    # ç”Ÿæˆ VLM è¾“å…¥æç¤º
    vlm_prompt = f"""# å¹»ç¯ç‰‡ç¬¬ {slide_num} é¡µç»¼åˆåˆ†æ

## å·²æå–çš„æ–‡æœ¬å†…å®¹ï¼ˆé«˜å¯ä¿¡åº¦ï¼Œæ¥è‡ªPPTåŸå§‹æ•°æ®ï¼‰ï¼š

### æ ‡é¢˜
{extracted_text['title'] or 'æ— '}

### æ­£æ–‡å†…å®¹
"""
    for i, body in enumerate(extracted_text['body'], 1):
        vlm_prompt += f"{i}. {body}\n"
    
    if extracted_text['tables']:
        vlm_prompt += "\n### è¡¨æ ¼\n"
        for t_idx, table in enumerate(extracted_text['tables'], 1):
            vlm_prompt += f"è¡¨æ ¼ {t_idx}:\n"
            for row in table[:3]:
                vlm_prompt += f"  {' | '.join(row)}\n"
    
    if extracted_text['notes']:
        vlm_prompt += f"\n### å¤‡æ³¨\n{extracted_text['notes']}\n"
    
    vlm_prompt += f"""

## å›¾ç‰‡OCRç»“æœï¼ˆ{len(image_ocr_results)} å¼ å›¾ç‰‡ï¼‰ï¼š
"""
    for ocr_res in image_ocr_results:
        vlm_prompt += f"- å›¾ç‰‡ {ocr_res['image_id']}: {ocr_res['text_blocks_count']} ä¸ªæ–‡æœ¬å—\n"
    
    vlm_prompt += """

## VLM ä»»åŠ¡ï¼š
è¯·ç»¼åˆä¸Šè¿°ä¿¡æ¯ï¼Œç”Ÿæˆå®Œæ•´çš„é¡µé¢å†…å®¹æè¿°ï¼š
1. ç¡®è®¤å¹¶æ•´åˆå·²æå–çš„æ–‡æœ¬
2. è¡¥å……å›¾ç‰‡ä¸­çš„é¢å¤–ä¿¡æ¯ï¼ˆå›¾è¡¨ã€ç¤ºæ„å›¾ã€å›¾æ ‡ç­‰ï¼‰
3. è¯†åˆ«é¡µé¢çš„æ•´ä½“ç±»å‹å’Œä¸»é¢˜
4. æå–å…³é”®ä¿¡æ¯å’Œç»“æ„

è¿”å› JSON æ ¼å¼ã€‚
"""
    
    vlm_prompt_path = output_dir / f"page_{slide_num:03d}_vlm_prompt.txt"
    with open(vlm_prompt_path, 'w', encoding='utf-8') as f:
        f.write(vlm_prompt)
    
    # è¿™é‡Œå¯ä»¥è°ƒç”¨ VLMï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    # æš‚æ—¶ä¿å­˜æç¤ºå’Œä¸Šä¸‹æ–‡
    vlm_context_path = output_dir / f"page_{slide_num:03d}_vlm_context.json"
    with open(vlm_context_path, 'w', encoding='utf-8') as f:
        json.dump(vlm_context, f, ensure_ascii=False, indent=2)
    
    print(f"    âœ“ VLMä¸Šä¸‹æ–‡å’Œæç¤ºå·²ä¿å­˜")
    
    # ==================== æ„å»ºæœ€ç»ˆé¡µé¢æ•°æ® ====================
    # æ•´åˆæ‰€æœ‰æ–‡æœ¬
    all_text = []
    if extracted_text['title']:
        all_text.append(extracted_text['title'])
    all_text.extend(extracted_text['body'])
    if extracted_text['notes']:
        all_text.append(f"å¤‡æ³¨: {extracted_text['notes']}")
    
    # æ·»åŠ è¡¨æ ¼æ–‡æœ¬
    for table in extracted_text['tables']:
        for row in table:
            all_text.append(' | '.join(row))
    
    # æ·»åŠ å›¾ç‰‡OCRæ–‡æœ¬ï¼ˆåªä¿ç•™é«˜ç½®ä¿¡åº¦ï¼‰
    MIN_CONFIDENCE = 0.85
    for ocr_res in image_ocr_results:
        try:
            with open(ocr_res['ocr_json'], 'r', encoding='utf-8') as f:
                ocr_data = json.load(f)
                # ä» text_blocks æå–é«˜ç½®ä¿¡åº¦æ–‡æœ¬
                if ocr_data.get('text_blocks'):
                    high_confidence_blocks = [
                        block for block in ocr_data['text_blocks']
                        if block.get('confidence', 0) >= MIN_CONFIDENCE and block.get('text')
                    ]
                    if high_confidence_blocks:
                        image_texts = [block['text'] for block in high_confidence_blocks]
                        all_text.append(f"[å›¾ç‰‡ {ocr_res['image_id']}-é«˜ç½®ä¿¡åº¦] " + ' '.join(image_texts))
                    
                    # è®°å½•è¿‡æ»¤æƒ…å†µ
                    low_conf_count = len(ocr_data['text_blocks']) - len(high_confidence_blocks)
                    if low_conf_count > 0:
                        print(f"      â„¹ï¸  å›¾ç‰‡ {ocr_res['image_id']}: è¿‡æ»¤äº† {low_conf_count} ä¸ªä½ç½®ä¿¡åº¦æ–‡æœ¬å—")
        except Exception as e:
            print(f"      âš ï¸  æ— æ³•è¯»å–å›¾ç‰‡OCRç»“æœ: {e}")
    
    combined_text = '\n\n'.join(all_text)
    
    # ç»Ÿè®¡ä¿¡æ¯
    slide_data['statistics'] = {
        "total_text_blocks": len(all_text),
        "total_images": len(images),
        "has_title": bool(extracted_text['title']),
        "has_tables": len(extracted_text['tables']) > 0,
        "has_notes": bool(extracted_text['notes'])
    }
    
    # Stage1 ä¿¡æ¯ï¼ˆæ¨¡æ‹Ÿ PDF çš„ç»“æ„ï¼‰
    # ä½¿ç”¨ç¬¬ä¸€å¼ æå–çš„å›¾ç‰‡ä½œä¸ºé¢„è§ˆå›¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    preview_image = f"page_{slide_num:03d}_preview.png"
    if images:
        # ä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºé¢„è§ˆ
        first_image_path = Path(images[0]['path'])
        preview_image = first_image_path.name
    
    slide_data['stage1_global'] = {
        "image": preview_image,
        "ocr_json": str(text_json_path),
        "text_source": "direct_extraction"
    }
    
    # Stage3 VLM ä¿¡æ¯
    slide_data['stage3_vlm'] = {
        "vlm_prompt": str(vlm_prompt_path),
        "vlm_context": str(vlm_context_path),
        "text_combined": combined_text
    }
    
    return slide_data, combined_text


def process_pptx(pptx_path, output_dir, ocr_engine='paddle'):
    """
    å®Œæ•´å¤„ç† PPTX æ–‡ä»¶
    ç”Ÿæˆä¸ adaptive_ocr_pipeline.py ç›¸åŒçš„è¾“å‡ºç»“æ„
    """
    print(f"ğŸš€ å¼€å§‹å¤„ç† PPTX: {pptx_path}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”§ OCRå¼•æ“: {ocr_engine}")
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    prs = Presentation(pptx_path)
    total_slides = len(prs.slides)
    
    print(f"ğŸ“„ æ€»é¡µæ•°: {total_slides}")
    
    result = {
        "source_file": str(pptx_path),
        "file_type": "pptx",
        "total_pages": total_slides,
        "ocr_engine": ocr_engine,
        "pages": []
    }
    
    for slide_idx, slide in enumerate(prs.slides, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ“„ å¤„ç†ç¬¬ {slide_idx}/{total_slides} é¡µ")
        print(f"{'='*70}")
        
        slide_data, combined_text = extract_slide_content(
            slide, slide_idx, output_dir, ocr_engine
        )
        
        result["pages"].append(slide_data)
        
        print(f"\nâœ… ç¬¬ {slide_idx} é¡µå®Œæˆ")
        print(f"  - æ–‡æœ¬å—: {slide_data['statistics']['total_text_blocks']}")
        print(f"  - å›¾ç‰‡: {slide_data['statistics']['total_images']}")
        print(f"  - å­—ç¬¦æ•°: {len(combined_text)}")
    
    # ä¿å­˜å®Œæ•´ç»“æœï¼ˆä¸ PDF çš„ complete_adaptive_ocr.json æ ¼å¼ä¸€è‡´ï¼‰
    complete_json = output_dir / "complete_adaptive_ocr.json"
    with open(complete_json, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*70}")
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"  - æ€»é¡µæ•°: {total_slides}")
    print(f"  - è¾“å‡ºæ–‡ä»¶: {complete_json}")
    print(f"  - è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    
    return result


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="PPTX å®Œæ•´å¤„ç†ç®¡é“")
    parser.add_argument("input_file", help="è¾“å…¥ PPTX æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", default=None, help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼špptx_outputï¼‰")
    parser.add_argument("--ocr-engine", choices=['easy', 'paddle', 'vision'], 
                       default='paddle', help="OCRå¼•æ“é€‰æ‹©")
    
    args = parser.parse_args()
    
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        sys.exit(1)
    
    if args.output:
        output_dir = Path(args.output)
    else:
        # é»˜è®¤è¾“å‡ºç›®å½•ï¼šæ–‡ä»¶å_adaptive
        output_dir = Path(input_path.stem.replace(' ', '_') + "_adaptive")
    
    try:
        result = process_pptx(input_path, output_dir, args.ocr_engine)
        print(f"\nğŸ‰ æˆåŠŸï¼å¯ä»¥ä½¿ç”¨æ­¤è¾“å‡ºç›®å½•é›†æˆåˆ°ç³»ç»Ÿä¸­ã€‚")
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


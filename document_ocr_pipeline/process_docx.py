#!/usr/bin/env python3
"""
DOCX å®Œæ•´å¤„ç†ç®¡é“ (åŸºäº PDFPlumber é‡æ„ç‰ˆ)
æ–¹æ¡ˆ Bï¼šDOCX -> PDF -> PDFPlumber é€é¡µæå–
ä¼˜åŠ¿ï¼š
1. ç²¾ç¡®çš„ç‰©ç†åˆ†é¡µ (Page-aware)
2. è¡¨æ ¼å®šä½å‡†ç¡® (Table-aware)
3. Markdown æ ¼å¼è¾“å‡º (LLM-friendly)
4. ç»Ÿä¸€çš„ PDF å¤„ç†é€»è¾‘
"""

import sys
import json
import argparse
from pathlib import Path
import io
import subprocess
import shutil
import base64

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from document_ocr_pipeline.extract_document import DocumentExtractor
from document_ocr_pipeline.visualize_extraction import visualize_extraction
from src.utils import get_soffice_command

try:
    from src.models import VisionModel
except ImportError:
    print("âš ï¸ Warning: Could not import VisionModel. VLM features will be disabled.")
    VisionModel = None

def encode_image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def refine_page_with_vlm(image_path, xml_text, ocr_text, vlm_model):
    """
    ä½¿ç”¨ VLM æ™ºèƒ½é‡ç»„é¡µé¢å†…å®¹ï¼šä»¥ XML æ–‡æœ¬ä¸ºéª¨æ¶ï¼Œå°† OCR è¯†åˆ«çš„å›¾ç‰‡å†…å®¹æ’å…¥æ­£ç¡®ä½ç½®
    """
    if not vlm_model:
        return None

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å†…å®¹ä¿®å¤ä¸“å®¶ã€‚
æˆ‘å°†æä¾›ä¸€é¡µæ–‡æ¡£çš„æˆªå›¾ã€é€šè¿‡ä»£ç æå–çš„ç²¾å‡†æ–‡æœ¬ï¼ˆXML Textï¼‰ä»¥åŠ OCR è¯†åˆ«çš„è¡¥å……æ–‡æœ¬ï¼ˆOCR Textï¼‰ã€‚

ã€æ ¸å¿ƒä»»åŠ¡ã€‘
ä½ çš„ç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä»½å†…å®¹å®Œæ•´ã€å‡†ç¡®çš„æ–‡æ¡£ã€‚è¯·éµå¾ªä»¥ä¸‹**ä¸¥æ ¼çš„åŒé‡æ ‡å‡†**ï¼š

1. **é’ˆå¯¹ XML Textï¼ˆéª¨æ¶éƒ¨åˆ†ï¼‰**ï¼š
   - ğŸ›¡ï¸ **ç»å¯¹å†»ç»“**ï¼šè¿™æ˜¯ä»æ–‡æ¡£æºç ç›´æ¥æå–çš„ï¼Œå…·æœ‰æœ€é«˜ä¼˜å…ˆçº§ã€‚
   - ğŸš« **ç¦æ­¢ä¿®æ”¹**ï¼šå³ä½¿ä½ å‘ç°æ‹¼å†™é”™è¯¯æˆ–æ ¼å¼é—®é¢˜ï¼Œä¹Ÿ**ç»å¯¹ä¸è¦ä¿®æ”¹**ä»»ä½•å­—ç¬¦ã€‚å¿…é¡»åŸæ ·ä¿ç•™ã€‚

2. **é’ˆå¯¹ OCR Textï¼ˆå›¾ç‰‡å†…å®¹éƒ¨åˆ†ï¼‰**ï¼š
   - ğŸ©¹ **æ™ºèƒ½ä¿®å¤**ï¼šè¿™æ˜¯ä»å›¾ç‰‡è¯†åˆ«çš„ï¼Œå¯èƒ½åŒ…å«è¯†åˆ«é”™è¯¯ã€‚
   - âœ¨ **çº é”™æŒ‡ä»¤**ï¼šåœ¨å°† OCR å†…å®¹æ’å…¥ XML éª¨æ¶ä¹‹å‰ï¼Œè¯·ç»“åˆå›¾ç‰‡è§†è§‰ä¿¡æ¯å’Œä½ çš„çŸ¥è¯†åº“ï¼Œ**ä¿®å¤æ˜æ˜¾çš„ OCR é”™è¯¯**ã€‚
     - é‡ç‚¹å…³æ³¨ï¼šæŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚ Elasticseatch â†’ Elasticsearchï¼‰ã€å“ç‰Œåç§°ï¼ˆå¦‚ Kibaha â†’ Kibanaï¼‰ã€æ ‡ç‚¹ç¬¦å·ã€‚
     - ä¸è¦è¿‡åº¦è”æƒ³ï¼Œåªä¿®æ­£è‚‰çœ¼å¯è§çš„æ˜æ˜¾é”™è¯¯ã€‚

ã€æ“ä½œæ­¥éª¤ã€‘
1. ä»¥ XML Text ä¸ºåŸºç¡€ï¼Œä¿æŒå…¶ç»“æ„ä¸åŠ¨ã€‚
2. ä» OCR Text ä¸­æå–å‡º XML Text ç¼ºå¤±çš„å›¾ç‰‡/æ’å›¾æ–‡å­—ã€‚
3. å¯¹æå–å‡ºçš„ OCR æ–‡å­—è¿›è¡Œ**æ™ºèƒ½çº é”™**ã€‚
4. å°†çº é”™åçš„å†…å®¹æ’å…¥åˆ° XML Text çš„æ­£ç¡®è§†è§‰ä½ç½®ï¼ˆå‚è€ƒ Imageï¼‰ã€‚
5. è¾“å‡ºæœ€ç»ˆçš„å®Œæ•´ Markdown æ–‡æœ¬ã€‚

ã€XML Textã€‘
{xml_text}

ã€OCR Textã€‘
{ocr_text}

è¯·ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„åˆå¹¶æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€‚
"""
    try:
        print("    ğŸ¤– è°ƒç”¨ VLM è¿›è¡Œæ™ºèƒ½é‡ç»„...")
        base64_image = encode_image_to_base64(image_path)
        response = vlm_model.chat(prompt, [base64_image])
        return response
    except Exception as e:
        print(f"    âŒ VLM é‡ç»„å¤±è´¥: {e}")
        return None


def extract_table_to_markdown(table):
    """
    å°† pdfplumber æå–çš„è¡¨æ ¼è½¬æ¢ä¸º Markdown æ ¼å¼
    table: list of lists of strings
    """
    if not table:
        return ""
        
    # æ¸…ç†å•å…ƒæ ¼æ•°æ®ï¼šå»é™¤ Noneï¼Œå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œå¤„ç†æ¢è¡Œç¬¦
    cleaned_table = []
    for row in table:
        cleaned_row = []
        for cell in row:
            if cell is None:
                cleaned_cell = ""
            else:
                # æ›¿æ¢æ¢è¡Œç¬¦ä¸ºç©ºæ ¼ï¼Œé¿å…ç ´å Markdown è¡¨æ ¼ç»“æ„
                cleaned_cell = str(cell).strip().replace('\n', ' ')
            cleaned_row.append(cleaned_cell)
        cleaned_table.append(cleaned_row)
        
    if not cleaned_table:
        return ""
        
    markdown_lines = []
    
    # 1. è¡¨å¤´
    headers = cleaned_table[0]
    markdown_lines.append("| " + " | ".join(headers) + " |")
    
    # 2. åˆ†éš”çº¿
    markdown_lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
    
    # 3. æ•°æ®è¡Œ
    for row in cleaned_table[1:]:
        # ç¡®ä¿è¡Œé•¿åº¦ä¸€è‡´
        padded_row = row + [""] * (len(headers) - len(row))
        markdown_lines.append("| " + " | ".join(padded_row[:len(headers)]) + " |")
        
    return "\n".join(markdown_lines)

def process_docx(docx_path, output_dir, ocr_engine='paddle', use_vlm=True):
    """
    å®Œæ•´å¤„ç† DOCX æ–‡ä»¶ (é€šè¿‡ PDF ä¸­è½¬)
    """
    print(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£ (æ–¹æ¡ˆB: PDFä¸­è½¬): {docx_path}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”§ OCRå¼•æ“: {ocr_engine}")
    print(f"ğŸ§  VLMèåˆ: {'å¼€å¯' if use_vlm else 'å…³é—­'}")
    
    # åˆå§‹åŒ– VLM
    vlm_model = None
    if use_vlm and VisionModel:
        try:
            vlm_model = VisionModel()
            print("  âœ“ VLM æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"  âš ï¸ VLM åˆå§‹åŒ–å¤±è´¥: {e}")
            use_vlm = False
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    docx_path = Path(docx_path)
    
    # å®‰å…¨å¤„ç†ä¸­æ–‡æ–‡ä»¶åï¼šå¦‚æœåŒ…å«é ASCII å­—ç¬¦ï¼Œå…ˆå¤åˆ¶ä¸ºä¸´æ—¶æ–‡ä»¶
    temp_process_file = None
    try:
        original_docx_path = docx_path
        if any(ord(c) > 127 for c in str(docx_path)):
            import uuid
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ (ä½¿ç”¨å®‰å…¨çš„çº¯è‹±æ–‡å)
            temp_name = f"temp_process_{uuid.uuid4().hex}{docx_path.suffix}"
            temp_process_file = docx_path.parent / temp_name
            shutil.copy2(docx_path, temp_process_file)
            print(f"  ğŸ”„ ä½¿ç”¨å®‰å…¨ä¸´æ—¶æ–‡ä»¶å¤„ç†ä¸­æ–‡åæ–‡ä»¶: {docx_path.name} -> {temp_name}")
            docx_path = temp_process_file
            
        # ==================== æ­¥éª¤ 1: LibreOffice è½¬æ¢ DOCX -> PDF ====================
        print(f"\n{'='*70}")
        print(f"ğŸ“„ æ­¥éª¤ 1: è½¬æ¢ä¸º PDF (è·å–ç²¾å‡†å¸ƒå±€)")
        print(f"{'='*70}")
        
        temp_pdf = output_dir / f"{original_docx_path.stem}_temp.pdf"
        
        # è·å– LibreOffice å‘½ä»¤
        soffice_cmd = get_soffice_command()
        if not soffice_cmd:
            print("  âŒ é”™è¯¯: æœªæ‰¾åˆ° LibreOffice (soffice)ï¼Œæ— æ³•è¿›è¡Œè½¬æ¢")
            print("  è¯·å®‰è£… LibreOffice å¹¶ç¡®ä¿ soffice å‘½ä»¤åœ¨ PATH ä¸­ï¼Œæˆ–è®¾ç½® SOFFICE_PATH ç¯å¢ƒå˜é‡ã€‚")
            print("  macOS: brew install --cask libreoffice")
            print("  Ubuntu: sudo apt install libreoffice")
            return None

        try:
            print(f"  â³ è½¬æ¢æ–‡æ¡£ä¸º PDF (ä½¿ç”¨: {soffice_cmd})...")
            
            # å¯¹äºçº¯æ–‡æœ¬æ–‡ä»¶ (.txt, .md)ï¼Œæ˜¾å¼æŒ‡å®šè¿‡æ»¤å™¨ä»¥ç¡®ä¿æ­£ç¡®ç¼–ç å’Œæ¢è¡Œ
            convert_args = [
                soffice_cmd,
                '--headless',
                '--convert-to', 'pdf',
                '--infilter=Text (encoded):UTF8,LF,Liberation Mono,10', 
                '--outdir', str(output_dir),
                str(docx_path)
            ]
            
            # å¦‚æœæ˜¯æ–‡æœ¬æ–‡ä»¶ï¼ŒLibreOffice é»˜è®¤è¡Œä¸ºé€šå¸¸è¶³å¤Ÿå¥½ï¼Œä½†å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ è¿‡æ»¤å™¨
            # ä¾‹å¦‚: --infilter="Text (encoded):UTF8,LF,,," 
            # ä½†ç›®å‰ä¿æŒé»˜è®¤å³å¯ï¼ŒLibreOffice æ™ºèƒ½è¯†åˆ«èƒ½åŠ›å¾ˆå¼º
            
            subprocess.run(convert_args, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            # LibreOffice è¾“å‡ºçš„æ–‡ä»¶åå¤„ç†
            # æ³¨æ„ï¼šLibreOffice ä¼šä½¿ç”¨è¾“å…¥æ–‡ä»¶åç”Ÿæˆ PDF
            generated_pdf = output_dir / f"{docx_path.stem}.pdf"
            if generated_pdf.exists():
                generated_pdf.rename(temp_pdf)
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°é¢„æœŸæ–‡ä»¶ï¼Œå¯èƒ½æ˜¯å› ä¸ºä½¿ç”¨äº†åŸå§‹ä¸­æ–‡åï¼ˆå¦‚æœæ²¡æœ‰è¿›å…¥ä¸´æ—¶æ–‡ä»¶é€»è¾‘ï¼‰
                # å°è¯•æ‰¾åŸå§‹åç§°çš„ PDF
                orig_pdf = output_dir / f"{original_docx_path.stem}.pdf"
                if orig_pdf.exists():
                    orig_pdf.rename(temp_pdf)
            
            if not temp_pdf.exists():
                raise FileNotFoundError(f"PDF conversion failed, expected output not found: {temp_pdf}")
                
            print(f"  âœ“ PDF å·²ç”Ÿæˆ: {temp_pdf.name}")
            
        except Exception as e:
            print(f"  âŒ è½¬æ¢å¤±è´¥: {e}")
            return None
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_process_file and temp_process_file.exists():
            try:
                temp_process_file.unlink()
                print(f"  ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_process_file.name}")
            except Exception as e:
                print(f"  âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    # ==================== æ­¥éª¤ 2: åˆå§‹åŒ– OCR å¼•æ“ ====================
    print(f"\n{'='*70}")
    print(f"ğŸ“„ æ­¥éª¤ 2: åˆå§‹åŒ–å¼•æ“")
    print(f"{'='*70}")
    
    ocr_extractor = DocumentExtractor(use_layout_detection=False, ocr_engine=ocr_engine)
    print(f"  âœ“ OCR å¼•æ“å°±ç»ª: {ocr_engine}")
    
    # ==================== æ­¥éª¤ 3: é€é¡µå¤„ç† PDF ====================
    import pdfplumber
    import cv2
    import numpy as np

    pages_data = []
    total_paragraphs = 0
    total_tables = 0
    
    print(f"\n{'='*70}")
    print(f"ğŸ“„ æ­¥éª¤ 3: é€é¡µæå–å†…å®¹ (æ–‡æœ¬ + è¡¨æ ¼ + OCR)")
    print(f"{'='*70}")
        
    with pdfplumber.open(temp_pdf) as pdf:
        page_count = len(pdf.pages)
        print(f"  ğŸ“š æ€»é¡µæ•°: {page_count}")
        
        for page_num, page in enumerate(pdf.pages, 1):
            print(f"\nå¤„ç†ç¬¬ {page_num}/{page_count} é¡µ...")
            
            # ---------------- 3.1 ç”Ÿæˆé¢„è§ˆå›¾ (å‘½åä¿®æ­£ä¸º _300dpi.png ä»¥å…¼å®¹ PDF æµç¨‹) ----------------
            img = page.to_image(resolution=300)
            img_array = np.array(img.original)
            
            # å…³é”®ä¿®æ­£ï¼šå°† preview.png æ”¹ä¸º 300dpi.pngï¼Œè§£å†³å‰ç«¯/å¤§æ¨¡å‹ 404 é—®é¢˜
            preview_image = f"page_{page_num:03d}_300dpi.png"
            preview_path = output_dir / preview_image
            cv2.imwrite(str(preview_path), cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR))
            print(f"  ğŸ–¼ï¸  é¢„è§ˆå›¾: {preview_image}")
        
            # ---------------- 3.2 æå–æ–‡æœ¬ (High Quality) ----------------
            # layout=True å°è¯•ä¿æŒç‰©ç†å¸ƒå±€
            text_content = page.extract_text(layout=True) or ""
            para_count = len(text_content.split('\n')) if text_content else 0
            total_paragraphs += para_count
            print(f"  ğŸ“ æ–‡æœ¬æå–: {len(text_content)} å­—ç¬¦")
            
            # ---------------- 3.3 æå–è¡¨æ ¼ -> Markdown ----------------
            tables = page.extract_tables()
            table_md_list = []
            if tables:
                print(f"  ğŸ“Š å‘ç°è¡¨æ ¼: {len(tables)} ä¸ª")
                total_tables += len(tables)
                for tbl in tables:
                    md = extract_table_to_markdown(tbl)
                    if md:
                        table_md_list.append(md)
    
            # ---------------- 3.4 å‡†å¤‡ XML åŸºç¡€æ–‡æœ¬ ----------------
            xml_base_text = text_content
            
            if table_md_list:
                table_section = "\n\nã€è¡¨æ ¼æ•°æ® (Markdown)ã€‘\n" + "\n\n".join(table_md_list)
                xml_base_text += table_section
                print(f"    âœ“ å·²è½¬æ¢ {len(table_md_list)} ä¸ªè¡¨æ ¼ä¸º Markdown")
                
            # ---------------- 3.5 OCR è¡¥å…… (é’ˆå¯¹å›¾ç‰‡/æ‰«æä»¶) ----------------
            # æ™ºèƒ½åˆ¤æ–­ç­–ç•¥ï¼šå¦‚æœé¡µé¢æ²¡æœ‰å›¾ç‰‡ï¼Œç›´æ¥ä» PDF è·å–åæ ‡ï¼Œè·³è¿‡ OCR å’Œ VLM
            has_images = len(page.images) > 0
            
            # è·å– PDF åŸç”Ÿåæ ‡ï¼ˆå³ä½¿æ— å›¾ä¹Ÿéœ€è¦ï¼Œä½œä¸ºé«˜ç²¾åº¦æ–‡æœ¬å®šä½ï¼‰
            # pdfplumber words æ ¼å¼: {'text': 'foo', 'x0': 10, 'top': 20, 'x1': 30, 'bottom': 40, ...}
            # éœ€è¦è½¬æ¢ä¸º bbox [x1, y1, x2, y2] å¹¶æŒ‰æ¯”ä¾‹æ”¾å¤§åˆ° 300 DPI
            pdf_words = page.extract_words()
            pdf_bbox_list = []
            
            # åæ ‡æ¢ç®—ç³»æ•°ï¼šPDFç‚¹(72DPI) -> å›¾ç‰‡åƒç´ (300DPI)
            SCALE_FACTOR = 300 / 72  # 4.1666...
            
            for w in pdf_words:
                pdf_bbox_list.append({
                    "text": w['text'],
                    "bbox": [
                        int(w['x0'] * SCALE_FACTOR),
                        int(w['top'] * SCALE_FACTOR),
                        int(w['x1'] * SCALE_FACTOR),
                        int(w['bottom'] * SCALE_FACTOR)
                    ],
                    "confidence": 1.0  # PDF åŸç”Ÿæ–‡æœ¬ç½®ä¿¡åº¦ä¸º 100%
                })
            
            ocr_full_text = ""
            avg_confidence = 1.0
            ocr_text_blocks = []

            if not has_images and len(text_content) > 0:
                print(f"    â© [æ™ºèƒ½ç­–ç•¥] çº¯æ–‡æœ¬é¡µé¢ (æ— å›¾ç‰‡)ï¼Œä½¿ç”¨ PDF åŸç”Ÿåæ ‡ï¼Œè·³è¿‡ OCR")
                # ä½¿ç”¨è½¬æ¢åçš„ PDF åæ ‡ä¼ªè£…æˆ OCR ç»“æœ
                ocr_result = {"text_blocks": pdf_bbox_list}
                ocr_text_blocks = pdf_bbox_list
                # ä¿å­˜ä¼ªè£…çš„ OCR JSON
                page_ocr_json = output_dir / f"page_{page_num:03d}_global_ocr.json"
                with open(page_ocr_json, 'w', encoding='utf-8') as f:
                    json.dump(ocr_result, f, ensure_ascii=False, indent=2)
                
                # ç”Ÿæˆå¯è§†åŒ– (å¯é€‰ï¼Œä¸ºäº†è°ƒè¯•)
                vis_path = output_dir / f"page_{page_num:03d}_visualized.png"
                # å¯¹äºçº¯æ–‡æœ¬ï¼Œç›´æ¥å¤åˆ¶åŸå›¾ä½œä¸ºå¯è§†åŒ–å›¾ï¼Œæˆ–è€…è·³è¿‡ç”»æ¡†ä»¥èŠ‚çœæ—¶é—´
                import shutil
                shutil.copy(preview_path, vis_path)
                
            else:
                # æœ‰å›¾ç‰‡ï¼Œæˆ–è€…è™½ç„¶æ²¡å›¾ä½†ä¹Ÿæ²¡æå–åˆ°æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯çº¯æ‰«æä»¶ä½† image å¯¹è±¡è¢«å°è£…äº†ï¼‰
                # æ‰§è¡Œå®Œæ•´çš„ OCR æµç¨‹
                print(f"  ğŸ” è¿è¡Œ OCR (å‘ç° {len(page.images)} å¼ å›¾ç‰‡)...")
                try:
                    ocr_result = ocr_extractor.extract_from_image(str(preview_path))
                    
                    # ä¿å­˜ OCR JSON
                    page_ocr_json = output_dir / f"page_{page_num:03d}_global_ocr.json"
                    with open(page_ocr_json, 'w', encoding='utf-8') as f:
                        json.dump(ocr_result, f, ensure_ascii=False, indent=2)
                        
                    # ç”Ÿæˆå¯è§†åŒ–
                    vis_path = output_dir / f"page_{page_num:03d}_visualized.png"
                    visualize_extraction(str(preview_path), str(page_ocr_json), str(vis_path))
                    
                    # æå– OCR æ–‡æœ¬
                    ocr_text_blocks = ocr_result.get('text_blocks', [])
                    ocr_texts = [b.get('text', '') for b in ocr_text_blocks if b.get('text', '').strip()]
                    ocr_full_text = "\n".join(ocr_texts)
                    
                    if ocr_text_blocks:
                        confs = [b.get('confidence', 0) for b in ocr_text_blocks]
                        avg_confidence = sum(confs) / len(confs)
                        
                except Exception as e:
                    print(f"  âŒ OCR å‡ºé”™: {e}")
                    ocr_text_blocks = []

            # ---------------- 3.6 æ™ºèƒ½èåˆ (XML + OCR + VLM) ----------------
            final_page_text = ""
            vlm_success = False
            
            # æ™ºèƒ½ VLM è§¦å‘é€»è¾‘ï¼š
            # 1. å¿…é¡»æœ‰å›¾ç‰‡ (has_images = True)
            # 2. å¿…é¡»å¯ç”¨äº† VLM
            # 3. OCR å¿…é¡»è¯†åˆ«åˆ°äº†å†…å®¹
            should_use_vlm = use_vlm and vlm_model and has_images and len(ocr_full_text) > 20
            
            if should_use_vlm:
                print("  ğŸ§  å°è¯•ä½¿ç”¨ VLM è¿›è¡Œå†…å®¹èåˆ...")
                refined_text = refine_page_with_vlm(str(preview_path), xml_base_text, ocr_full_text, vlm_model)
                if refined_text:
                    final_page_text = refined_text
                    vlm_success = True
                    print("    âœ“ VLM èåˆæˆåŠŸ")
            elif not has_images:
                 print("    â© [æ™ºèƒ½ç­–ç•¥] çº¯æ–‡æœ¬é¡µé¢ï¼Œè·³è¿‡ VLM")
            
            # å¦‚æœ VLM æœªå¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„å›é€€ç­–ç•¥
            if not final_page_text:
                if vlm_model: 
                    print("    âš ï¸ VLM æœªè¿”å›ç»“æœï¼Œå›é€€åˆ°ä¼ ç»Ÿæ‹¼æ¥æ¨¡å¼")
                
                final_page_text = xml_base_text
                # æ™ºèƒ½åˆå¹¶ç­–ç•¥ï¼š
                # å¦‚æœç›´æ¥æå–çš„æ–‡æœ¬å¾ˆå°‘ï¼Œè¯´æ˜å¯èƒ½æ˜¯çº¯å›¾ï¼Œä½¿ç”¨ OCR æ–‡æœ¬ä½œä¸ºä¸»åŠ›
                if len(xml_base_text) < 50 and len(ocr_full_text) > 50:
                    print("    âš ï¸  é¡µé¢æ–‡æœ¬æå°‘ï¼Œé‡‡ç”¨ OCR ç»“æœä¸ºä¸»")
                    final_page_text = f"{final_page_text}\n\nã€OCR è¯†åˆ«å†…å®¹ã€‘\n{ocr_full_text}"
                elif len(ocr_full_text) > 0:
                    # å¦åˆ™ä½œä¸ºè¡¥å……
                    final_page_text += f"\n\nã€è§†è§‰è¯†åˆ«è¡¥å…… (OCR)ã€‘\n{ocr_full_text}"

            # ---------------- 3.7 æ„å»º Page Data ----------------
            page_data = {
                "page_number": page_num,
                "statistics": {
                    "total_characters": len(final_page_text),
                    "total_tables": len(tables),
                    "avg_ocr_confidence": round(avg_confidence, 3)
                },
                "stage1_global": {
                    "image": preview_image,
                    "text_source": "xml+vlm" if vlm_success else "xml+ocr_fallback"
                },
                "stage3_vlm": {
                    "text_combined": final_page_text,
                    "vlm_refined": vlm_success
                }
            }
            pages_data.append(page_data)

    # ==================== æ­¥éª¤ 4: ç”Ÿæˆè¾“å‡ºæ–‡ä»¶ ====================
    
    # 1. complete_adaptive_ocr.json (å…¼å®¹æ—§æ ¼å¼)
    result = {
        "source_file": str(docx_path),
        "file_type": "docx",
        "total_pages": page_count,
        "ocr_engine": ocr_engine,
        "pages": pages_data,
        "statistics": {
            "total_paragraphs": total_paragraphs,
            "total_tables": total_tables
        }
    }
    
    complete_json = output_dir / "complete_adaptive_ocr.json"
    with open(complete_json, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # 2. complete_document.json (ES ç´¢å¼•æ ¼å¼)
    pages_for_index = []
    for page in pages_data:
        page_num = page['page_number']
        text_combined = page['stage3_vlm']['text_combined']
        image_filename = page['stage1_global']['image']
        
        pages_for_index.append({
            'page_number': page_num,
            'image_path': str(output_dir / image_filename),
            'image_filename': image_filename,
            'content': {
                'full_text_cleaned': text_combined,
                'full_text_raw': text_combined,
                'key_fields': [],
                'tables': [] # ç»“æ„åŒ–æ•°æ®åç»­å¯æ‰©å±•
            },
            'ocr_data': {
                'text_blocks': []
            },
            'metadata': {
                'extraction_method': 'docx_via_pdfplumber',
                'ocr_engine': ocr_engine,
                'avg_ocr_confidence': page['statistics']['avg_ocr_confidence'],
                'vlm_refined': page['stage3_vlm'].get('vlm_refined', False)
            }
        })
        
    complete_document_path = output_dir / "complete_document.json"
    with open(complete_document_path, 'w', encoding='utf-8') as f:
        json.dump({'pages': pages_for_index}, f, ensure_ascii=False, indent=2)

    # æ¸…ç†ä¸´æ—¶ PDF
    if temp_pdf.exists():
        temp_pdf.unlink()
        
    print(f"\n{'='*70}")
    print(f"âœ… å¤„ç†å®Œæˆ (æ–¹æ¡ˆB)")
    print(f"ğŸ“Š ç»Ÿè®¡: {page_count} é¡µ, {total_tables} ä¸ªè¡¨æ ¼")
    print(f"ğŸ“‚ è¾“å‡º: {output_dir}")
    print(f"{'='*70}")
    
    return result

def main():
    parser = argparse.ArgumentParser(description='Process DOCX via PDF conversion')
    parser.add_argument('docx_file', help='Path to DOCX file')
    parser.add_argument('-o', '--output', help='Output directory', default=None)
    parser.add_argument('--ocr-engine', choices=['easy', 'paddle', 'vision'], 
                       default='paddle', help='OCR engine to use')
    parser.add_argument('--no-vlm', action='store_true', help='Disable VLM refinement')
    
    args = parser.parse_args()
    
    docx_path = Path(args.docx_file)
    if not docx_path.exists():
        print(f"Error: File not found: {docx_path}")
        return 1
    
    if args.output:
        output_dir = Path(args.output)
    else:
        output_dir = Path(f"{docx_path.stem}_docx_processed")
    
    try:
        process_docx(docx_path, output_dir, args.ocr_engine, use_vlm=not args.no_vlm)
        return 0
    except Exception as e:
        print(f"âŒ Fatal Error: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == '__main__':
    sys.exit(main())

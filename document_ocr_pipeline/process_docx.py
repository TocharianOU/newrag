#!/usr/bin/env python3
"""
DOCX å®Œæ•´å¤„ç†ç®¡é“
ç”Ÿæˆä¸ PDF æµç¨‹ä¸€è‡´çš„è¾“å‡ºç»“æ„
"""

import sys
import json
import argparse
from pathlib import Path
from docx import Document
from docx.oxml.table import CT_Tbl
from docx.oxml.text.paragraph import CT_P
from docx.table import _Cell, Table
from docx.text.paragraph import Paragraph
import io
from PIL import Image
import subprocess

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from document_ocr_pipeline.extract_document import DocumentExtractor
from document_ocr_pipeline.visualize_extraction import visualize_extraction


def extract_paragraph_content(doc, output_dir, ocr_engine='paddle'):
    """
    æå– Word æ–‡æ¡£å†…å®¹ï¼šæ–‡æœ¬ + è¡¨æ ¼ + å›¾ç‰‡OCR
    
    Word æ–‡æ¡£ç»“æ„ï¼š
    - æ®µè½ï¼ˆParagraphï¼‰
    - è¡¨æ ¼ï¼ˆTableï¼‰
    - å›¾ç‰‡ï¼ˆåµŒå…¥åœ¨æ®µè½æˆ–è¡¨æ ¼ä¸­ï¼‰
    
    ç­–ç•¥ï¼š
    1. æŒ‰é¡ºåºéå†æ–‡æ¡£å…ƒç´ 
    2. æå–æ–‡æœ¬å’Œè¡¨æ ¼
    3. æå–å›¾ç‰‡å¹¶è¿›è¡Œ OCR
    4. åˆå¹¶é«˜ç½®ä¿¡åº¦ç»“æœ
    """
    
    content_data = {
        "paragraphs": [],
        "tables": [],
        "images": [],
        "image_ocr_results": []
    }
    
    # ==================== é˜¶æ®µ 1: æå–æ–‡æœ¬å’Œè¡¨æ ¼ ====================
    print(f"\n  ğŸ“ é˜¶æ®µ1: æå–æ–‡æ¡£å†…å®¹ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰...")
    
    # æå–æ®µè½
    for para in doc.paragraphs:
        text = para.text.strip()
        if text:
            content_data["paragraphs"].append(text)
    
    print(f"    âœ“ æ®µè½: {len(content_data['paragraphs'])} ä¸ª")
    
    # æå–è¡¨æ ¼
    for table_idx, table in enumerate(doc.tables, 1):
        table_data = []
        for row in table.rows:
            row_data = [cell.text.strip() for cell in row.cells]
            table_data.append(row_data)
        content_data["tables"].append(table_data)
    
    print(f"    âœ“ è¡¨æ ¼: {len(content_data['tables'])} ä¸ª")
    
    # ==================== é˜¶æ®µ 2: æå–å›¾ç‰‡å¹¶ OCR ====================
    print(f"  ğŸ–¼ï¸  é˜¶æ®µ2: å¤„ç†å›¾ç‰‡å†…å®¹...")
    
    # ä»æ–‡æ¡£ä¸­æå–æ‰€æœ‰å›¾ç‰‡
    image_count = 0
    for rel_id, rel in doc.part.rels.items():
        if "image" in rel.target_ref:
            try:
                image_count += 1
                image_data = rel.target_part.blob
                
                # æ£€æµ‹å›¾ç‰‡æ ¼å¼
                img = Image.open(io.BytesIO(image_data))
                img_format = img.format.lower() if img.format else 'png'
                
                image_filename = f"image_{image_count:03d}.{img_format}"
                image_path = output_dir / image_filename
                
                # ä¿å­˜å›¾ç‰‡
                with open(image_path, "wb") as f:
                    f.write(image_data)
                
                width, height = img.size
                
                content_data["images"].append({
                    "id": image_count,
                    "path": str(image_path),
                    "format": img_format,
                    "size": [width, height]
                })
                
                print(f"    âœ“ å›¾ç‰‡ {image_count}: {width}x{height} ({img_format})")
                
                # å¯¹å›¾ç‰‡è¿è¡Œ OCR
                ocr_json_path = output_dir / f"image_{image_count:03d}_ocr.json"
                try:
                    extractor = DocumentExtractor(ocr_engine=ocr_engine)
                    ocr_result = extractor.extract_from_image(str(image_path))
                    
                    with open(ocr_json_path, 'w', encoding='utf-8') as f:
                        json.dump(ocr_result, f, ensure_ascii=False, indent=2)
                    
                    # ç”Ÿæˆå¯è§†åŒ–
                    vis_path = output_dir / f"image_{image_count:03d}_visualized.png"
                    visualize_extraction(str(image_path), str(ocr_json_path), str(vis_path))
                    
                    content_data["image_ocr_results"].append({
                        "image_id": image_count,
                        "ocr_json": str(ocr_json_path),
                        "visualized": str(vis_path),
                        "text_blocks_count": len(ocr_result.get('text_blocks', []))
                    })
                    
                    print(f"      âœ“ OCR: {len(ocr_result.get('text_blocks', []))} ä¸ªæ–‡æœ¬å—")
                except Exception as e:
                    print(f"      âœ— OCRå¤±è´¥: {e}")
                    
            except Exception as e:
                print(f"    âœ— å›¾ç‰‡ {image_count} æå–å¤±è´¥: {e}")
    
    # ==================== é˜¶æ®µ 3: åˆå¹¶æ–‡æœ¬ï¼ˆç½®ä¿¡åº¦è¿‡æ»¤ï¼‰ ====================
    print(f"  ğŸ¤– é˜¶æ®µ3: åˆå¹¶æ–‡æœ¬å†…å®¹...")
    
    # åˆå¹¶æ®µè½æ–‡æœ¬
    direct_text = "\n\n".join(content_data["paragraphs"])
    
    # åˆå¹¶è¡¨æ ¼æ–‡æœ¬
    if content_data["tables"]:
        table_text = "\n\n".join([
            "\n".join([" | ".join(row) for row in table])
            for table in content_data["tables"]
        ])
        direct_text += f"\n\nã€è¡¨æ ¼å†…å®¹ã€‘\n{table_text}"
    
    # åˆå¹¶é«˜ç½®ä¿¡åº¦ OCR æ–‡æœ¬
    ocr_texts = []
    low_confidence_count = 0
    
    for ocr_result in content_data["image_ocr_results"]:
        ocr_json_path = Path(ocr_result["ocr_json"])
        if ocr_json_path.exists():
            with open(ocr_json_path, 'r', encoding='utf-8') as f:
                ocr_data = json.load(f)
            
            for block in ocr_data.get('text_blocks', []):
                confidence = block.get('confidence', 0.0)
                text = block.get('text', '').strip()
                
                if confidence >= 0.85 and text:
                    ocr_texts.append(text)
                elif text:
                    low_confidence_count += 1
    
    combined_text = direct_text
    if ocr_texts:
        combined_text += f"\n\nã€å›¾ç‰‡æ–‡å­—ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰ã€‘\n" + "\n".join(ocr_texts)
    
    if low_confidence_count > 0:
        print(f"    â„¹ï¸  è¿‡æ»¤äº† {low_confidence_count} ä¸ªä½ç½®ä¿¡åº¦æ–‡æœ¬å—")
    
    print(f"    âœ“ åˆå¹¶æ–‡æœ¬å®Œæˆ: {len(combined_text)} å­—ç¬¦")
    
    # ä¿å­˜æå–ç»“æœ
    text_json_path = output_dir / "extracted_content.json"
    with open(text_json_path, 'w', encoding='utf-8') as f:
        json.dump({
            "paragraphs": content_data["paragraphs"],
            "tables": content_data["tables"],
            "images": [img["path"] for img in content_data["images"]],
            "combined_text": combined_text
        }, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆ VLM ä¸Šä¸‹æ–‡
    vlm_context_path = output_dir / "vlm_context.json"
    vlm_context = {
        "paragraph_count": len(content_data["paragraphs"]),
        "table_count": len(content_data["tables"]),
        "image_count": len(content_data["images"]),
        "ocr_results": content_data["image_ocr_results"]
    }
    
    with open(vlm_context_path, 'w', encoding='utf-8') as f:
        json.dump(vlm_context, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆ VLM æç¤º
    vlm_prompt = f"""# Word æ–‡æ¡£å†…å®¹åˆ†æ

## æ–‡æ¡£ç»“æ„
- æ®µè½æ•°: {len(content_data['paragraphs'])}
- è¡¨æ ¼æ•°: {len(content_data['tables'])}
- å›¾ç‰‡æ•°: {len(content_data['images'])}

## æ–‡æœ¬å†…å®¹
{direct_text[:2000]}...

## OCR æå–çš„å›¾ç‰‡æ–‡å­—
{chr(10).join(ocr_texts[:10]) if ocr_texts else 'æ— '}

---
è¯·åˆ†ææ–‡æ¡£å†…å®¹å¹¶æå–å…³é”®ä¿¡æ¯ã€‚
"""
    
    vlm_prompt_path = output_dir / "vlm_prompt.txt"
    with open(vlm_prompt_path, 'w', encoding='utf-8') as f:
        f.write(vlm_prompt)
    
    return {
        "text_combined": combined_text,
        "statistics": {
            "total_paragraphs": len(content_data["paragraphs"]),
            "total_tables": len(content_data["tables"]),
            "total_images": len(content_data["images"]),
            "total_characters": len(combined_text)
        },
        "extracted_content_json": str(text_json_path),
        "vlm_context": str(vlm_context_path),
        "vlm_prompt": str(vlm_prompt_path),
        "images": content_data["images"]
    }


def process_docx(docx_path, output_dir, ocr_engine='paddle'):
    """
    å®Œæ•´å¤„ç† DOCX æ–‡ä»¶
    ç”Ÿæˆä¸ adaptive_ocr_pipeline.py ç›¸åŒçš„è¾“å‡ºç»“æ„
    """
    print(f"ğŸš€ å¼€å§‹å¤„ç† DOCX: {docx_path}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”§ OCRå¼•æ“: {ocr_engine}")
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    docx_path = Path(docx_path)
    
    # ==================== æ­¥éª¤ 0: ä½¿ç”¨ LibreOffice è½¬æ¢ä¸º PDF å¹¶æ¸²æŸ“é¢„è§ˆå›¾ ====================
    print(f"\n{'='*70}")
    print(f"ğŸ“„ æ­¥éª¤ 0: ç”Ÿæˆé¡µé¢é¢„è§ˆå›¾ï¼ˆLibreOffice æ¸²æŸ“ï¼‰")
    print(f"{'='*70}")
    
    temp_pdf = output_dir / f"{docx_path.stem}_temp.pdf"
    page_count = 0
    
    try:
        # è°ƒç”¨ LibreOffice è½¬æ¢ DOCX -> PDF
        print(f"  â³ è½¬æ¢ DOCX ä¸º PDF...")
        subprocess.run([
            '/Applications/LibreOffice.app/Contents/MacOS/soffice',
            '--headless',
            '--convert-to', 'pdf',
            '--outdir', str(output_dir),
            str(docx_path)
        ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        # LibreOffice è¾“å‡ºçš„ PDF æ–‡ä»¶åä¸è¾“å…¥æ–‡ä»¶åç›¸åŒï¼ˆä»…æ‰©å±•åä¸åŒï¼‰
        generated_pdf = output_dir / f"{docx_path.stem}.pdf"
        if generated_pdf.exists() and generated_pdf != temp_pdf:
            generated_pdf.rename(temp_pdf)
        
        print(f"  âœ“ PDF å·²ç”Ÿæˆ: {temp_pdf.name}")
        
        # ä½¿ç”¨ pdfplumber æ¸²æŸ“æ¯ä¸€é¡µä¸ºå›¾ç‰‡
        import pdfplumber
        import cv2
        import numpy as np
        
        with pdfplumber.open(temp_pdf) as pdf:
            page_count = len(pdf.pages)
            print(f"  ğŸ“„ PDF é¡µæ•°: {page_count}")
            
            for page_num, page in enumerate(pdf.pages, 1):
                # æ¸²æŸ“ä¸ºé«˜è´¨é‡å›¾ç‰‡ï¼ˆ300 DPIï¼‰
                img = page.to_image(resolution=300)
                img_array = np.array(img.original)
                
                # ä¿å­˜ä¸º page_XXX_preview.pngï¼ˆä¸ PDF æµç¨‹å‘½åä¸€è‡´ï¼‰
                preview_path = output_dir / f"page_{page_num:03d}_preview.png"
                cv2.imwrite(str(preview_path), cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR))
                
                height, width = img_array.shape[:2]
                print(f"  âœ“ ç¬¬ {page_num} é¡µ: {width}x{height}px -> {preview_path.name}")
        
        # åˆ é™¤ä¸´æ—¶ PDF æ–‡ä»¶
        temp_pdf.unlink()
        print(f"  âœ“ é¢„è§ˆå›¾ç”Ÿæˆå®Œæˆï¼Œä¸´æ—¶ PDF å·²æ¸…ç†")
        
    except FileNotFoundError:
        print("  âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° LibreOfficeï¼Œè·³è¿‡é¢„è§ˆå›¾ç”Ÿæˆ")
        print("  æç¤º: å®‰è£… LibreOffice ä»¥å¯ç”¨é¡µé¢é¢„è§ˆåŠŸèƒ½")
        print("  macOS: brew install --cask libreoffice")
    except Exception as e:
        print(f"  âš ï¸  é¢„è§ˆå›¾ç”Ÿæˆå¤±è´¥: {e}")
    
    # ==================== ç»§ç»­åŸæœ‰çš„å†…å®¹æå–æµç¨‹ ====================
    doc = Document(str(docx_path))
    
    print(f"\n{'='*70}")
    print(f"ğŸ“„ å¤„ç† Word æ–‡æ¡£å†…å®¹")
    print(f"{'='*70}")
    
    # æå–å†…å®¹
    extraction_result = extract_paragraph_content(doc, output_dir, ocr_engine)
    
    # ==================== æ„å»ºè¾“å‡ºç»“æ„ï¼ˆæ¨¡æ‹Ÿ PDF çš„ complete_adaptive_ocr.jsonï¼‰ ====================
    # Word æ–‡æ¡£é€šå¸¸æ˜¯å•é¡µçš„é€»è¾‘ç»“æ„ï¼Œä½†å¯èƒ½æœ‰å¤šä¸ªç‰©ç†é¡µé¢
    # æˆ‘ä»¬å°†æ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ª"é¡µé¢"å¤„ç†
    
    preview_image = "page_001_preview.png"
    preview_path = output_dir / preview_image
    
    # å¦‚æœé¢„è§ˆå›¾ä¸å­˜åœ¨ä¸”æœ‰æå–çš„å›¾ç‰‡ï¼Œä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡
    if not preview_path.exists() and extraction_result["images"]:
        first_image_path = Path(extraction_result["images"][0]['path'])
        preview_image = first_image_path.name
    
    # Stage2 OCR å¯è§†åŒ–ä¿¡æ¯
    visualized_image = "page_001_visualized.png"
    visualized_path = output_dir / visualized_image
    
    if preview_path.exists():
        # ç›´æ¥å¤åˆ¶é¢„è§ˆå›¾ä½œä¸ºå¯è§†åŒ–ç»“æœ
        import shutil
        shutil.copy2(preview_path, visualized_path)
    elif extraction_result["images"]:
        # ä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡çš„å¯è§†åŒ–
        first_vis = extraction_result["images"][0]['path'].replace('.', '_visualized.')
        if Path(first_vis).exists():
            import shutil
            shutil.copy2(first_vis, visualized_path)
    
    page_data = {
        "page_number": 1,
        "statistics": extraction_result["statistics"],
        "stage1_global": {
            "image": preview_image,
            "ocr_json": extraction_result["extracted_content_json"],
            "text_source": "direct_extraction"
        },
        "stage2_ocr": {
            "ocr_json": extraction_result["extracted_content_json"],
            "visualized": str(visualized_path) if visualized_path.exists() else ""
        },
        "stage3_vlm": {
            "vlm_prompt": extraction_result["vlm_prompt"],
            "vlm_context": extraction_result["vlm_context"],
            "text_combined": extraction_result["text_combined"]
        }
    }
    
    result = {
        "source_file": str(docx_path),
        "file_type": "docx",
        "total_pages": max(page_count, 1),  # ä½¿ç”¨å®é™… PDF é¡µæ•°ï¼Œæˆ–è‡³å°‘ä¸º 1
        "ocr_engine": ocr_engine,
        "pages": [page_data]
    }
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    complete_json = output_dir / "complete_adaptive_ocr.json"
    with open(complete_json, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*70}")
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"  - ç‰©ç†é¡µæ•°: {page_count}")
    print(f"  - æ®µè½æ•°: {extraction_result['statistics']['total_paragraphs']}")
    print(f"  - è¡¨æ ¼æ•°: {extraction_result['statistics']['total_tables']}")
    print(f"  - å›¾ç‰‡æ•°: {extraction_result['statistics']['total_images']}")
    print(f"  - å­—ç¬¦æ•°: {extraction_result['statistics']['total_characters']}")
    print(f"  - è¾“å‡ºæ–‡ä»¶: {complete_json}")
    print(f"  - è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    
    return result


def main():
    parser = argparse.ArgumentParser(description='Process DOCX file with OCR and VLM')
    parser.add_argument('docx_file', help='Path to DOCX file')
    parser.add_argument('-o', '--output', help='Output directory', default=None)
    parser.add_argument('--ocr-engine', choices=['easy', 'paddle', 'vision'], 
                       default='paddle', help='OCR engine to use')
    
    args = parser.parse_args()
    
    docx_path = Path(args.docx_file)
    if not docx_path.exists():
        print(f"Error: DOCX file not found: {docx_path}")
        return 1
    
    # ç”Ÿæˆè¾“å‡ºç›®å½•å
    if args.output:
        output_dir = Path(args.output)
    else:
        output_dir = Path(f"{docx_path.stem}_docx_processed")
    
    try:
        process_docx(docx_path, output_dir, args.ocr_engine)
        return 0
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())





#!/usr/bin/env python3
"""
DOCX å®Œæ•´å¤„ç†ç®¡é“ (åŸºäº PDFPlumber é‡æ„ç‰ˆ)
æ–¹æ¡ˆ Bï¼šDOCX -> PDF -> PDFPlumber é€é¡µæå–
ä¼˜åŠ¿ï¼š
1. ç²¾ç¡®çš„ç‰©ç†åˆ†é¡µ (Page-aware)
2. è¡¨æ ¼å®šä½å‡†ç¡® (Table-aware)
3. Markdown æ ¼å¼è¾“å‡º (LLM-friendly)
4. ç»Ÿä¸€çš„ PDF å¤„ç†é€»è¾‘
"""

import sys
import json
import argparse
from pathlib import Path
import io
import subprocess
import shutil
import base64

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from document_ocr_pipeline.extract_document import DocumentExtractor
from document_ocr_pipeline.visualize_extraction import visualize_extraction
try:
    from src.models import VisionModel
except ImportError:
    print("âš ï¸ Warning: Could not import VisionModel. VLM features will be disabled.")
    VisionModel = None

def encode_image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def refine_page_with_vlm(image_path, xml_text, ocr_text, vlm_model):
    """
    ä½¿ç”¨ VLM æ™ºèƒ½é‡ç»„é¡µé¢å†…å®¹ï¼šä»¥ XML æ–‡æœ¬ä¸ºéª¨æ¶ï¼Œå°† OCR è¯†åˆ«çš„å›¾ç‰‡å†…å®¹æ’å…¥æ­£ç¡®ä½ç½®
    """
    if not vlm_model:
        return None

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å†…å®¹ä¿®å¤ä¸“å®¶ã€‚
æˆ‘å°†æä¾›ä¸€é¡µæ–‡æ¡£çš„æˆªå›¾ã€é€šè¿‡ä»£ç æå–çš„ç²¾å‡†æ–‡æœ¬ï¼ˆXML Textï¼‰ä»¥åŠ OCR è¯†åˆ«çš„è¡¥å……æ–‡æœ¬ï¼ˆOCR Textï¼‰ã€‚

ã€æ ¸å¿ƒä»»åŠ¡ã€‘
ä½ çš„ç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä»½å†…å®¹å®Œæ•´ã€å‡†ç¡®çš„æ–‡æ¡£ã€‚è¯·éµå¾ªä»¥ä¸‹**ä¸¥æ ¼çš„åŒé‡æ ‡å‡†**ï¼š

1. **é’ˆå¯¹ XML Textï¼ˆéª¨æ¶éƒ¨åˆ†ï¼‰**ï¼š
   - ğŸ›¡ï¸ **ç»å¯¹å†»ç»“**ï¼šè¿™æ˜¯ä»æ–‡æ¡£æºç ç›´æ¥æå–çš„ï¼Œå…·æœ‰æœ€é«˜ä¼˜å…ˆçº§ã€‚
   - ğŸš« **ç¦æ­¢ä¿®æ”¹**ï¼šå³ä½¿ä½ å‘ç°æ‹¼å†™é”™è¯¯æˆ–æ ¼å¼é—®é¢˜ï¼Œä¹Ÿ**ç»å¯¹ä¸è¦ä¿®æ”¹**ä»»ä½•å­—ç¬¦ã€‚å¿…é¡»åŸæ ·ä¿ç•™ã€‚

2. **é’ˆå¯¹ OCR Textï¼ˆå›¾ç‰‡å†…å®¹éƒ¨åˆ†ï¼‰**ï¼š
   - ğŸ©¹ **æ™ºèƒ½ä¿®å¤**ï¼šè¿™æ˜¯ä»å›¾ç‰‡è¯†åˆ«çš„ï¼Œå¯èƒ½åŒ…å«è¯†åˆ«é”™è¯¯ã€‚
   - âœ¨ **çº é”™æŒ‡ä»¤**ï¼šåœ¨å°† OCR å†…å®¹æ’å…¥ XML éª¨æ¶ä¹‹å‰ï¼Œè¯·ç»“åˆå›¾ç‰‡è§†è§‰ä¿¡æ¯å’Œä½ çš„çŸ¥è¯†åº“ï¼Œ**ä¿®å¤æ˜æ˜¾çš„ OCR é”™è¯¯**ã€‚
     - é‡ç‚¹å…³æ³¨ï¼šæŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚ Elasticseatch â†’ Elasticsearchï¼‰ã€å“ç‰Œåç§°ï¼ˆå¦‚ Kibaha â†’ Kibanaï¼‰ã€æ ‡ç‚¹ç¬¦å·ã€‚
     - ä¸è¦è¿‡åº¦è”æƒ³ï¼Œåªä¿®æ­£è‚‰çœ¼å¯è§çš„æ˜æ˜¾é”™è¯¯ã€‚

ã€æ“ä½œæ­¥éª¤ã€‘
1. ä»¥ XML Text ä¸ºåŸºç¡€ï¼Œä¿æŒå…¶ç»“æ„ä¸åŠ¨ã€‚
2. ä» OCR Text ä¸­æå–å‡º XML Text ç¼ºå¤±çš„å›¾ç‰‡/æ’å›¾æ–‡å­—ã€‚
3. å¯¹æå–å‡ºçš„ OCR æ–‡å­—è¿›è¡Œ**æ™ºèƒ½çº é”™**ã€‚
4. å°†çº é”™åçš„å†…å®¹æ’å…¥åˆ° XML Text çš„æ­£ç¡®è§†è§‰ä½ç½®ï¼ˆå‚è€ƒ Imageï¼‰ã€‚
5. è¾“å‡ºæœ€ç»ˆçš„å®Œæ•´ Markdown æ–‡æœ¬ã€‚

ã€XML Textã€‘
{xml_text}

ã€OCR Textã€‘
{ocr_text}

è¯·ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„åˆå¹¶æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€‚
"""
    try:
        print("    ğŸ¤– è°ƒç”¨ VLM è¿›è¡Œæ™ºèƒ½é‡ç»„...")
        base64_image = encode_image_to_base64(image_path)
        response = vlm_model.chat(prompt, [base64_image])
        return response
    except Exception as e:
        print(f"    âŒ VLM é‡ç»„å¤±è´¥: {e}")
        return None


def extract_table_to_markdown(table):
    """
    å°† pdfplumber æå–çš„è¡¨æ ¼è½¬æ¢ä¸º Markdown æ ¼å¼
    table: list of lists of strings
    """
    if not table:
        return ""
        
    # æ¸…ç†å•å…ƒæ ¼æ•°æ®ï¼šå»é™¤ Noneï¼Œå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œå¤„ç†æ¢è¡Œç¬¦
    cleaned_table = []
    for row in table:
        cleaned_row = []
        for cell in row:
            if cell is None:
                cleaned_cell = ""
            else:
                # æ›¿æ¢æ¢è¡Œç¬¦ä¸ºç©ºæ ¼ï¼Œé¿å…ç ´å Markdown è¡¨æ ¼ç»“æ„
                cleaned_cell = str(cell).strip().replace('\n', ' ')
            cleaned_row.append(cleaned_cell)
        cleaned_table.append(cleaned_row)
        
    if not cleaned_table:
        return ""
        
    markdown_lines = []
    
    # 1. è¡¨å¤´
    headers = cleaned_table[0]
    markdown_lines.append("| " + " | ".join(headers) + " |")
    
    # 2. åˆ†éš”çº¿
    markdown_lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
    
    # 3. æ•°æ®è¡Œ
    for row in cleaned_table[1:]:
        # ç¡®ä¿è¡Œé•¿åº¦ä¸€è‡´
        padded_row = row + [""] * (len(headers) - len(row))
        markdown_lines.append("| " + " | ".join(padded_row[:len(headers)]) + " |")
        
    return "\n".join(markdown_lines)

def process_docx(docx_path, output_dir, ocr_engine='paddle', use_vlm=True):
    """
    å®Œæ•´å¤„ç† DOCX æ–‡ä»¶ (é€šè¿‡ PDF ä¸­è½¬)
    """
    print(f"ğŸš€ å¼€å§‹å¤„ç† DOCX (æ–¹æ¡ˆB: PDFä¸­è½¬): {docx_path}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”§ OCRå¼•æ“: {ocr_engine}")
    print(f"ğŸ§  VLMèåˆ: {'å¼€å¯' if use_vlm else 'å…³é—­'}")
    
    # åˆå§‹åŒ– VLM
    vlm_model = None
    if use_vlm and VisionModel:
        try:
            vlm_model = VisionModel()
            print("  âœ“ VLM æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"  âš ï¸ VLM åˆå§‹åŒ–å¤±è´¥: {e}")
            use_vlm = False
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    docx_path = Path(docx_path)
    
    # ==================== æ­¥éª¤ 1: LibreOffice è½¬æ¢ DOCX -> PDF ====================
    print(f"\n{'='*70}")
    print(f"ğŸ“„ æ­¥éª¤ 1: è½¬æ¢ä¸º PDF (è·å–ç²¾å‡†å¸ƒå±€)")
    print(f"{'='*70}")
    
    temp_pdf = output_dir / f"{docx_path.stem}_temp.pdf"
    
    try:
        print(f"  â³ è½¬æ¢ DOCX ä¸º PDF...")
        subprocess.run([
            '/Applications/LibreOffice.app/Contents/MacOS/soffice',
            '--headless',
            '--convert-to', 'pdf',
            '--outdir', str(output_dir),
            str(docx_path)
        ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        # LibreOffice è¾“å‡ºçš„æ–‡ä»¶åå¤„ç†
        generated_pdf = output_dir / f"{docx_path.stem}.pdf"
        if generated_pdf.exists() and generated_pdf != temp_pdf:
            generated_pdf.rename(temp_pdf)
        
        print(f"  âœ“ PDF å·²ç”Ÿæˆ: {temp_pdf.name}")
        
    except FileNotFoundError:
        print("  âŒ é”™è¯¯: æœªæ‰¾åˆ° LibreOfficeï¼Œæ— æ³•è¿›è¡Œè½¬æ¢")
        print("  è¯·å®‰è£…: brew install --cask libreoffice")
        return None
    except Exception as e:
        print(f"  âŒ è½¬æ¢å¤±è´¥: {e}")
        return None

    # ==================== æ­¥éª¤ 2: åˆå§‹åŒ– OCR å¼•æ“ ====================
    print(f"\n{'='*70}")
    print(f"ğŸ“„ æ­¥éª¤ 2: åˆå§‹åŒ–å¼•æ“")
    print(f"{'='*70}")
    
    ocr_extractor = DocumentExtractor(use_layout_detection=False, ocr_engine=ocr_engine)
    print(f"  âœ“ OCR å¼•æ“å°±ç»ª: {ocr_engine}")
    
    # ==================== æ­¥éª¤ 3: é€é¡µå¤„ç† PDF ====================
    import pdfplumber
    import cv2
    import numpy as np

    pages_data = []
    total_paragraphs = 0
    total_tables = 0
    
    print(f"\n{'='*70}")
    print(f"ğŸ“„ æ­¥éª¤ 3: é€é¡µæå–å†…å®¹ (æ–‡æœ¬ + è¡¨æ ¼ + OCR)")
    print(f"{'='*70}")
        
    with pdfplumber.open(temp_pdf) as pdf:
        page_count = len(pdf.pages)
        print(f"  ğŸ“š æ€»é¡µæ•°: {page_count}")
        
        for page_num, page in enumerate(pdf.pages, 1):
            print(f"\nå¤„ç†ç¬¬ {page_num}/{page_count} é¡µ...")
            
            # ---------------- 3.1 ç”Ÿæˆé¢„è§ˆå›¾ (å‘½åä¿®æ­£ä¸º _300dpi.png ä»¥å…¼å®¹ PDF æµç¨‹) ----------------
            img = page.to_image(resolution=300)
            img_array = np.array(img.original)
            
            # å…³é”®ä¿®æ­£ï¼šå°† preview.png æ”¹ä¸º 300dpi.pngï¼Œè§£å†³å‰ç«¯/å¤§æ¨¡å‹ 404 é—®é¢˜
            preview_image = f"page_{page_num:03d}_300dpi.png"
            preview_path = output_dir / preview_image
            cv2.imwrite(str(preview_path), cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR))
            print(f"  ğŸ–¼ï¸  é¢„è§ˆå›¾: {preview_image}")
        
            # ---------------- 3.2 æå–æ–‡æœ¬ (High Quality) ----------------
            # layout=True å°è¯•ä¿æŒç‰©ç†å¸ƒå±€
            text_content = page.extract_text(layout=True) or ""
            para_count = len(text_content.split('\n')) if text_content else 0
            total_paragraphs += para_count
            print(f"  ğŸ“ æ–‡æœ¬æå–: {len(text_content)} å­—ç¬¦")
            
            # ---------------- 3.3 æå–è¡¨æ ¼ -> Markdown ----------------
            tables = page.extract_tables()
            table_md_list = []
            if tables:
                print(f"  ğŸ“Š å‘ç°è¡¨æ ¼: {len(tables)} ä¸ª")
                total_tables += len(tables)
                for tbl in tables:
                    md = extract_table_to_markdown(tbl)
                    if md:
                        table_md_list.append(md)
    
            # ---------------- 3.4 å‡†å¤‡ XML åŸºç¡€æ–‡æœ¬ ----------------
            xml_base_text = text_content
            
            if table_md_list:
                table_section = "\n\nã€è¡¨æ ¼æ•°æ® (Markdown)ã€‘\n" + "\n\n".join(table_md_list)
                xml_base_text += table_section
                print(f"    âœ“ å·²è½¬æ¢ {len(table_md_list)} ä¸ªè¡¨æ ¼ä¸º Markdown")
                
            # ---------------- 3.5 OCR è¡¥å…… (é’ˆå¯¹å›¾ç‰‡/æ‰«æä»¶) ----------------
            print(f"  ğŸ” è¿è¡Œ OCR...")
            ocr_full_text = ""
            avg_confidence = 0.0
            
            try:
                ocr_result = ocr_extractor.extract_from_image(str(preview_path))
                
                # ä¿å­˜ OCR JSON (ä½¿ç”¨ _global_ocr.json å‘½åä»¥å…¼å®¹å‰ç«¯ bbox åŒ¹é…)
                page_ocr_json = output_dir / f"page_{page_num:03d}_global_ocr.json"
                with open(page_ocr_json, 'w', encoding='utf-8') as f:
                    json.dump(ocr_result, f, ensure_ascii=False, indent=2)
                    
                # ç”Ÿæˆå¯è§†åŒ–
                vis_path = output_dir / f"page_{page_num:03d}_visualized.png"
                visualize_extraction(str(preview_path), str(page_ocr_json), str(vis_path))
                
                # æå– OCR æ–‡æœ¬
                ocr_text_blocks = ocr_result.get('text_blocks', [])
                ocr_texts = [b.get('text', '') for b in ocr_text_blocks if b.get('text', '').strip()]
                ocr_full_text = "\n".join(ocr_texts)
                
                if ocr_text_blocks:
                    confs = [b.get('confidence', 0) for b in ocr_text_blocks]
                    avg_confidence = sum(confs) / len(confs)
                    
            except Exception as e:
                print(f"  âŒ OCR å‡ºé”™: {e}")
                ocr_text_blocks = []

            # ---------------- 3.6 æ™ºèƒ½èåˆ (XML + OCR + VLM) ----------------
            final_page_text = ""
            vlm_success = False
            
            # æ¡ä»¶ï¼šå¯ç”¨äº† VLMï¼Œä¸” OCR è¯†åˆ«åˆ°äº†å†…å®¹ï¼Œä¸” OCR å†…å®¹æ¯” XML å†…å®¹å¤šæˆ–è€…ç›¸å½“ï¼ˆè¯´æ˜æœ‰å›¾ç‰‡æ–‡å­—ï¼‰
            # æˆ–è€…åªè¦æœ‰ OCR å†…å®¹æˆ‘ä»¬å°±å°è¯•èåˆï¼Œè®© VLM å†³å®šæ˜¯å¦éœ€è¦è¡¥å……
            if use_vlm and vlm_model and len(ocr_full_text) > 20:
                print("  ğŸ§  å°è¯•ä½¿ç”¨ VLM è¿›è¡Œå†…å®¹èåˆ...")
                refined_text = refine_page_with_vlm(str(preview_path), xml_base_text, ocr_full_text, vlm_model)
                if refined_text:
                    final_page_text = refined_text
                    vlm_success = True
                    print("    âœ“ VLM èåˆæˆåŠŸ")
            
            # å¦‚æœ VLM æœªå¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„å›é€€ç­–ç•¥
            if not final_page_text:
                if vlm_model: 
                    print("    âš ï¸ VLM æœªè¿”å›ç»“æœï¼Œå›é€€åˆ°ä¼ ç»Ÿæ‹¼æ¥æ¨¡å¼")
                
                final_page_text = xml_base_text
                # æ™ºèƒ½åˆå¹¶ç­–ç•¥ï¼š
                # å¦‚æœç›´æ¥æå–çš„æ–‡æœ¬å¾ˆå°‘ï¼Œè¯´æ˜å¯èƒ½æ˜¯çº¯å›¾ï¼Œä½¿ç”¨ OCR æ–‡æœ¬ä½œä¸ºä¸»åŠ›
                if len(xml_base_text) < 50 and len(ocr_full_text) > 50:
                    print("    âš ï¸  é¡µé¢æ–‡æœ¬æå°‘ï¼Œé‡‡ç”¨ OCR ç»“æœä¸ºä¸»")
                    final_page_text = f"{final_page_text}\n\nã€OCR è¯†åˆ«å†…å®¹ã€‘\n{ocr_full_text}"
                elif len(ocr_full_text) > 0:
                    # å¦åˆ™ä½œä¸ºè¡¥å……
                    final_page_text += f"\n\nã€è§†è§‰è¯†åˆ«è¡¥å…… (OCR)ã€‘\n{ocr_full_text}"

            # ---------------- 3.7 æ„å»º Page Data ----------------
            page_data = {
                "page_number": page_num,
                "statistics": {
                    "total_characters": len(final_page_text),
                    "total_tables": len(tables),
                    "avg_ocr_confidence": round(avg_confidence, 3)
                },
                "stage1_global": {
                    "image": preview_image,
                    "text_source": "xml+vlm" if vlm_success else "xml+ocr_fallback"
                },
                "stage3_vlm": {
                    "text_combined": final_page_text,
                    "vlm_refined": vlm_success
                }
            }
            pages_data.append(page_data)

    # ==================== æ­¥éª¤ 4: ç”Ÿæˆè¾“å‡ºæ–‡ä»¶ ====================
    
    # 1. complete_adaptive_ocr.json (å…¼å®¹æ—§æ ¼å¼)
    result = {
        "source_file": str(docx_path),
        "file_type": "docx",
        "total_pages": page_count,
        "ocr_engine": ocr_engine,
        "pages": pages_data,
        "statistics": {
            "total_paragraphs": total_paragraphs,
            "total_tables": total_tables
        }
    }
    
    complete_json = output_dir / "complete_adaptive_ocr.json"
    with open(complete_json, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # 2. complete_document.json (ES ç´¢å¼•æ ¼å¼)
    pages_for_index = []
    for page in pages_data:
        page_num = page['page_number']
        text_combined = page['stage3_vlm']['text_combined']
        image_filename = page['stage1_global']['image']
        
        pages_for_index.append({
            'page_number': page_num,
            'image_path': str(output_dir / image_filename),
            'image_filename': image_filename,
            'content': {
                'full_text_cleaned': text_combined,
                'full_text_raw': text_combined,
                'key_fields': [],
                'tables': [] # ç»“æ„åŒ–æ•°æ®åç»­å¯æ‰©å±•
            },
            'ocr_data': {
                'text_blocks': []
            },
            'metadata': {
                'extraction_method': 'docx_via_pdfplumber',
                'ocr_engine': ocr_engine,
                'avg_ocr_confidence': page['statistics']['avg_ocr_confidence'],
                'vlm_refined': page['stage3_vlm'].get('vlm_refined', False)
            }
        })
        
    complete_document_path = output_dir / "complete_document.json"
    with open(complete_document_path, 'w', encoding='utf-8') as f:
        json.dump({'pages': pages_for_index}, f, ensure_ascii=False, indent=2)

    # æ¸…ç†ä¸´æ—¶ PDF
    if temp_pdf.exists():
        temp_pdf.unlink()
        
    print(f"\n{'='*70}")
    print(f"âœ… å¤„ç†å®Œæˆ (æ–¹æ¡ˆB)")
    print(f"ğŸ“Š ç»Ÿè®¡: {page_count} é¡µ, {total_tables} ä¸ªè¡¨æ ¼")
    print(f"ğŸ“‚ è¾“å‡º: {output_dir}")
    print(f"{'='*70}")
    
    return result

def main():
    parser = argparse.ArgumentParser(description='Process DOCX via PDF conversion')
    parser.add_argument('docx_file', help='Path to DOCX file')
    parser.add_argument('-o', '--output', help='Output directory', default=None)
    parser.add_argument('--ocr-engine', choices=['easy', 'paddle', 'vision'], 
                       default='paddle', help='OCR engine to use')
    parser.add_argument('--no-vlm', action='store_true', help='Disable VLM refinement')
    
    args = parser.parse_args()
    
    docx_path = Path(args.docx_file)
    if not docx_path.exists():
        print(f"Error: File not found: {docx_path}")
        return 1
    
    if args.output:
        output_dir = Path(args.output)
    else:
        output_dir = Path(f"{docx_path.stem}_docx_processed")
    
    try:
        process_docx(docx_path, output_dir, args.ocr_engine, use_vlm=not args.no_vlm)
        return 0
    except Exception as e:
        print(f"âŒ Fatal Error: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == '__main__':
    sys.exit(main())

#!/usr/bin/env python3
"""
PDF è½¬ç»“æ„åŒ– JSON ä¸€ä½“åŒ–è„šæœ¬ï¼ˆä½¿ç”¨ LM Studioï¼‰

å®Œæ•´æµç¨‹ï¼š
1. PDF è½¬å›¾ç‰‡
2. OCR æå–æ–‡æœ¬å’Œåæ ‡
3. ä½¿ç”¨ LM Studio VLM ç²¾ç‚¼ä¼˜åŒ–
4. è¾“å‡º [{}, {}] æ ¼å¼çš„ JSON åˆ—è¡¨

ä½¿ç”¨æ–¹æ³•ï¼š
    python pdf_to_json_gemini.py input.pdf output.json
"""

import os
import sys
import json
import base64
import argparse
from pathlib import Path
from typing import List, Dict, Any
import tempfile
import shutil

# PDF è½¬å›¾ç‰‡ä¾èµ–
try:
    from pdf2image import convert_from_path
    import cv2
    import numpy as np
    from PIL import Image
    from openai import OpenAI
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–åº“ï¼è¯·å®‰è£…ï¼š")
    print("   pip install pdf2image opencv-python numpy Pillow openai")
    sys.exit(1)

# OCR ä¾èµ–
try:
    import easyocr
    HAS_OCR = True
except ImportError:
    print("âš ï¸  è­¦å‘Šï¼šæœªå®‰è£… EasyOCRï¼Œå°†è·³è¿‡ OCR é˜¶æ®µ")
    HAS_OCR = False


class PDFToJSONProcessor:
    """PDF è½¬ç»“æ„åŒ– JSON å¤„ç†å™¨ï¼ˆä½¿ç”¨ LM Studioï¼‰"""
    
    # VLM æç¤ºè¯æ¨¡æ¿
    VLM_PROMPT = """ä½ æ˜¯ä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œæå–æ‰€æœ‰ä¿¡æ¯å¹¶ç”Ÿæˆç»“æ„åŒ– JSONã€‚

ã€å…³é”®è¦æ±‚ã€‘ï¼š
1. æå–æ–‡æ¡£å…ƒæ•°æ®ï¼ˆæ–‡æ¡£ç¼–å·ã€ç‰ˆæœ¬ã€é¡¹ç›®åç§°ã€å…¬å¸åç§°ç­‰ï¼‰
2. æå–è®¾å¤‡ä¿¡æ¯ï¼ˆè®¾å¤‡æ ‡ç­¾ã€åç§°ã€ç±»å‹ã€è§„æ ¼ç­‰ï¼‰
3. è¯†åˆ«å¹¶ç»“æ„åŒ–æ‰€æœ‰è¡¨æ ¼æ•°æ®
4. ä¿®æ­£ OCR é”™è¯¯ï¼ˆå¦‚æ—¥æœŸæ ¼å¼ï¼š15-58p-25 â†’ 15-Sep-25ï¼‰
5. æå–æ‰€æœ‰æŠ€æœ¯å‚æ•°å’Œå¤‡æ³¨
6. ç”Ÿæˆæœç´¢å…³é”®è¯

ã€è¾“å‡º JSON æ ¼å¼ã€‘ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
{
  "document_metadata": {
    "document_type": "æ–‡æ¡£ç±»å‹ï¼ˆå¦‚ Process Datasheetï¼‰",
    "document_number": "æ–‡æ¡£ç¼–å·",
    "revision": "ç‰ˆæœ¬å·",
    "project_name": "é¡¹ç›®åç§°",
    "plant": "å·¥å‚/è®¾æ–½åç§°",
    "equipment_tag": "è®¾å¤‡æ ‡ç­¾",
    "page": "å½“å‰é¡µç ",
    "total_pages": "æ€»é¡µæ•°"
  },
  "document_content": {
    "title": "æ–‡æ¡£æ ‡é¢˜",
    "equipment_name": "è®¾å¤‡åç§°",
    "process_unit": "å·¥è‰ºå•å…ƒ",
    "project_phase": "é¡¹ç›®é˜¶æ®µ",
    "package_number": "åŒ…å·",
    "area": "åŒºåŸŸ"
  },
  "revision_history": [
    {
      "revision": "ç‰ˆæœ¬å·",
      "date": "æ—¥æœŸï¼ˆä¿®æ­£åï¼‰",
      "description": "æè¿°",
      "prepared_by": "ç¼–åˆ¶äºº",
      "checked_by": "å®¡æ ¸äºº",
      "approved_by": "æ‰¹å‡†äºº"
    }
  ],
  "tables": [
    {
      "title": "è¡¨æ ¼æ ‡é¢˜",
      "headers": ["åˆ—å1", "åˆ—å2"],
      "rows": [["æ•°æ®1", "æ•°æ®2"]]
    }
  ],
  "technical_parameters": [
    {"parameter": "å‚æ•°å", "value": "å‚æ•°å€¼", "unit": "å•ä½"}
  ],
  "procedures": {
    "external_documentation": "å¤–éƒ¨æ–‡æ¡£è¦æ±‚",
    "review_acceptance_notes": ["å®¡æ ¸æ¥å—è¯´æ˜"]
  },
  "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
  "full_text_cleaned": "æ¸…æ´—åçš„å®Œæ•´æ–‡æœ¬",
  "extraction_notes": ["æå–è¿‡ç¨‹ä¸­çš„å¤‡æ³¨æˆ–ä¸ç¡®å®šé¡¹"]
}

ã€é‡è¦æç¤ºã€‘ï¼š
- ä¿®æ­£æ‰€æœ‰ OCR é”™è¯¯
- ä¿æŒåŸå§‹ä¿¡æ¯çš„å‡†ç¡®æ€§
- è¡¨æ ¼æ•°æ®å¿…é¡»å®Œæ•´æå–
- ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦ä»»ä½•è§£é‡Š
- å¦‚æœæŸä¸ªå­—æ®µæ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸² "" æˆ–ç©ºæ•°ç»„ []
"""
    
    def __init__(self, lm_studio_url: str = "http://localhost:1234/v1", 
                 model_name: str = "google/gemma-3-27b"):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            lm_studio_url: LM Studio API åœ°å€
            model_name: æ¨¡å‹åç§°
        """
        # é…ç½® LM Studio
        self.client = OpenAI(base_url=lm_studio_url, api_key="lm-studio")
        self.model_name = model_name
        
        # åˆå§‹åŒ– OCR
        self.ocr_reader = None
        if HAS_OCR:
            try:
                print("ğŸ”§ åˆå§‹åŒ– OCR å¼•æ“...")
                self.ocr_reader = easyocr.Reader(['en', 'ch_sim'], gpu=False)
                print("âœ“ OCR å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  OCR åˆå§‹åŒ–å¤±è´¥: {e}")
                self.ocr_reader = None
        
        print(f"âœ“ LM Studio å·²è¿æ¥: {lm_studio_url}")
    
    def pdf_to_images(self, pdf_path: str, dpi: int = 300) -> List[Path]:
        """
        å°† PDF è½¬æ¢ä¸ºå›¾ç‰‡
        
        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            dpi: å›¾ç‰‡åˆ†è¾¨ç‡
        
        Returns:
            å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        print(f"\nğŸ“„ æ­£åœ¨è½¬æ¢ PDF: {os.path.basename(pdf_path)}")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = Path(tempfile.mkdtemp(prefix="pdf_images_"))
        
        try:
            # è½¬æ¢ PDF
            images = convert_from_path(pdf_path, dpi=dpi, fmt='png')
            print(f"âœ“ PDF å…± {len(images)} é¡µ")
            
            # ä¿å­˜å›¾ç‰‡
            image_paths = []
            for i, image in enumerate(images, start=1):
                image_path = temp_dir / f"page_{i:03d}.png"
                image.save(image_path, 'PNG')
                image_paths.append(image_path)
                print(f"  [{i}/{len(images)}] {image_path.name}")
            
            return image_paths
            
        except Exception as e:
            print(f"âŒ PDF è½¬æ¢å¤±è´¥: {e}")
            shutil.rmtree(temp_dir, ignore_errors=True)
            raise
    
    def extract_text_with_ocr(self, image_path: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ OCR æå–æ–‡æœ¬å’Œåæ ‡
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
        
        Returns:
            OCR ç»“æœ
        """
        if not self.ocr_reader:
            return {
                "text_blocks": [],
                "full_text": "",
                "average_confidence": 0
            }
        
        print(f"  ğŸ” OCR è¯†åˆ«ä¸­...")
        
        # è¯»å–å›¾ç‰‡
        image = cv2.imread(str(image_path))
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # OCR æå–
        results = self.ocr_reader.readtext(image_rgb)
        
        # æ ¼å¼åŒ–ç»“æœ
        text_blocks = []
        full_text_parts = []
        
        for bbox, text, confidence in results:
            # è®¡ç®—è¾¹ç•Œæ¡†
            xs = [p[0] for p in bbox]
            ys = [p[1] for p in bbox]
            x1, y1, x2, y2 = min(xs), min(ys), max(xs), max(ys)
            
            text_blocks.append({
                "text": text.strip(),
                "bbox": [float(x1), float(y1), float(x2), float(y2)],
                "confidence": float(confidence),
                "center_y": float((y1 + y2) / 2),
                "center_x": float((x1 + x2) / 2)
            })
            full_text_parts.append(text.strip())
        
        # æŒ‰ä½ç½®æ’åº
        text_blocks.sort(key=lambda x: (x["center_y"], x["center_x"]))
        
        avg_confidence = sum(b["confidence"] for b in text_blocks) / len(text_blocks) if text_blocks else 0
        
        print(f"  âœ“ OCR å®Œæˆï¼š{len(text_blocks)} ä¸ªæ–‡æœ¬å—ï¼Œå¹³å‡ç½®ä¿¡åº¦ {avg_confidence*100:.1f}%")
        
        return {
            "text_blocks": text_blocks,
            "full_text": "\n".join(full_text_parts),
            "average_confidence": avg_confidence
        }
    
    def refine_with_vlm(self, image_path: str, ocr_data: Dict[str, Any], 
                       page_number: int, total_pages: int) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LM Studio VLM ç²¾ç‚¼ä¼˜åŒ–
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            ocr_data: OCR æ•°æ®
            page_number: å½“å‰é¡µç 
            total_pages: æ€»é¡µæ•°
        
        Returns:
            ç²¾ç‚¼åçš„ç»“æ„åŒ–æ•°æ®
        """
        print(f"  ğŸ¤– VLM åˆ†æä¸­...")
        
        # æ„å»ºå¢å¼ºæç¤ºè¯
        prompt = self.VLM_PROMPT
        
        if ocr_data.get("full_text"):
            prompt += f"\n\nã€OCR æå–çš„åŸå§‹æ–‡æœ¬ã€‘ï¼š\n{ocr_data['full_text']}\n"
            prompt += f"ã€OCR ç»Ÿè®¡ã€‘ï¼š{len(ocr_data.get('text_blocks', []))} ä¸ªæ–‡æœ¬å—ï¼Œå¹³å‡ç½®ä¿¡åº¦ {ocr_data.get('average_confidence', 0)*100:.1f}%\n"
        
        prompt += f"\nã€é¡µé¢ä¿¡æ¯ã€‘ï¼šç¬¬ {page_number} é¡µï¼Œå…± {total_pages} é¡µ\n"
        
        try:
            # ç¼–ç å›¾ç‰‡
            with open(image_path, "rb") as f:
                base64_image = base64.b64encode(f.read()).decode('utf-8')
            
            # è°ƒç”¨ LM Studio
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}},
                        {"type": "text", "text": prompt}
                    ]
                }],
                max_tokens=8192,
                temperature=0.1,
                stream=False
            )
            
            content = response.choices[0].message.content
            
            # æå– JSON
            json_start = content.find("{")
            json_end = content.rfind("}") + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = content[json_start:json_end]
                refined_data = json.loads(json_str)
                print(f"  âœ“ VLM åˆ†æå®Œæˆ")
                return refined_data
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON è¾“å‡º")
        
        except Exception as e:
            print(f"  âš ï¸  VLM åˆ†æå‡ºé”™: {e}")
            return {
                "document_metadata": {},
                "document_content": {},
                "revision_history": [],
                "tables": [],
                "technical_parameters": [],
                "procedures": {},
                "keywords": [],
                "full_text_cleaned": ocr_data.get("full_text", ""),
                "extraction_notes": [f"VLM åˆ†æå¤±è´¥: {str(e)}"]
            }
    
    def create_page_document(self, refined_data: Dict[str, Any], 
                            ocr_data: Dict[str, Any],
                            image_path: str,
                            page_number: int,
                            total_pages: int) -> Dict[str, Any]:
        """
        åˆ›å»ºå•é¡µæ–‡æ¡£ç»“æ„
        
        Args:
            refined_data: VLM ç²¾ç‚¼åçš„æ•°æ®
            ocr_data: OCR æ•°æ®
            image_path: å›¾ç‰‡è·¯å¾„
            page_number: é¡µç 
            total_pages: æ€»é¡µæ•°
        
        Returns:
            å•é¡µæ–‡æ¡£ç»“æ„
        """
        doc_metadata = refined_data.get('document_metadata', {})
        doc_content = refined_data.get('document_content', {})
        
        return {
            # ===== é¡µé¢æ ‡è¯† =====
            "page_number": page_number,
            "total_pages": total_pages,
            "source_image": os.path.basename(image_path),
            
            # ===== æ–‡æ¡£å…ƒæ•°æ® =====
            "document_id": doc_metadata.get('document_number', ''),
            "document_type": doc_metadata.get('document_type', ''),
            "revision": doc_metadata.get('revision', ''),
            
            # ===== é¡¹ç›®ä¿¡æ¯ =====
            "project": {
                "name": doc_metadata.get('project_name', ''),
                "plant": doc_metadata.get('plant', ''),
                "phase": doc_content.get('project_phase', '')
            },
            
            # ===== è®¾å¤‡ä¿¡æ¯ =====
            "equipment": {
                "tag": doc_metadata.get('equipment_tag', ''),
                "name": doc_content.get('equipment_name', ''),
                "title": doc_content.get('title', ''),
                "unit": doc_content.get('process_unit', ''),
                "area": doc_content.get('area', ''),
                "package": doc_content.get('package_number', '')
            },
            
            # ===== æ–‡æ¡£å†…å®¹ =====
            "content": {
                "full_text": refined_data.get('full_text_cleaned', ''),
                "full_text_raw": ocr_data.get('full_text', ''),
                "summary": doc_content.get('title', '')
            },
            
            # ===== ä¿®è®¢å†å² =====
            "revision_history": refined_data.get('revision_history', []),
            
            # ===== è¡¨æ ¼æ•°æ® =====
            "tables": refined_data.get('tables', []),
            
            # ===== æŠ€æœ¯å‚æ•° =====
            "technical_parameters": refined_data.get('technical_parameters', []),
            
            # ===== ç¨‹åºå’Œæµç¨‹ =====
            "procedures": refined_data.get('procedures', {}),
            
            # ===== æœç´¢å…³é”®è¯ =====
            "keywords": refined_data.get('keywords', []),
            
            # ===== OCR å…ƒæ•°æ® =====
            "ocr_metadata": {
                "text_blocks_count": len(ocr_data.get('text_blocks', [])),
                "average_confidence": ocr_data.get('average_confidence', 0)
            },
            
            # ===== æ–‡æœ¬å—åæ ‡ï¼ˆç”¨äºé«˜äº®ï¼‰ =====
            "text_blocks": [
                {
                    "text": block.get('text', ''),
                    "bbox": block.get('bbox', []),
                    "confidence": block.get('confidence', 0)
                }
                for block in ocr_data.get('text_blocks', [])
                if block.get('confidence', 0) > 0.3
            ],
            
            # ===== æå–æ³¨é‡Š =====
            "extraction_notes": refined_data.get('extraction_notes', [])
        }
    
    def process_pdf(self, pdf_path: str, output_json_path: str) -> List[Dict[str, Any]]:
        """
        å¤„ç† PDF æ–‡ä»¶ï¼Œç”Ÿæˆç»“æ„åŒ– JSON
        
        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            output_json_path: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„
        
        Returns:
            æ‰€æœ‰é¡µé¢çš„æ–‡æ¡£åˆ—è¡¨
        """
        print("\n" + "="*80)
        print("ğŸš€ PDF è½¬ç»“æ„åŒ– JSON å¤„ç†")
        print("="*80)
        
        # 1. PDF è½¬å›¾ç‰‡
        image_paths = self.pdf_to_images(pdf_path, dpi=300)
        total_pages = len(image_paths)
        
        # 2. å¤„ç†æ¯ä¸€é¡µ
        all_pages_data = []
        
        for i, image_path in enumerate(image_paths, start=1):
            print(f"\nğŸ“„ å¤„ç†ç¬¬ {i}/{total_pages} é¡µ...")
            
            try:
                # OCR æå–
                ocr_data = self.extract_text_with_ocr(str(image_path))
                
                # VLM ç²¾ç‚¼
                refined_data = self.refine_with_vlm(
                    str(image_path), 
                    ocr_data, 
                    i, 
                    total_pages
                )
                
                # åˆ›å»ºé¡µé¢æ–‡æ¡£
                page_doc = self.create_page_document(
                    refined_data, 
                    ocr_data, 
                    str(image_path),
                    i,
                    total_pages
                )
                
                all_pages_data.append(page_doc)
                print(f"  âœ… ç¬¬ {i} é¡µå¤„ç†å®Œæˆ")
                
            except Exception as e:
                print(f"  âŒ ç¬¬ {i} é¡µå¤„ç†å¤±è´¥: {e}")
                all_pages_data.append({
                    "page_number": i,
                    "total_pages": total_pages,
                    "error": str(e),
                    "document_id": "",
                    "content": {}
                })
        
        # 3. ä¿å­˜ JSON
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_json_path}")
        output_path = Path(output_json_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(all_pages_data, f, ensure_ascii=False, indent=2)
        
        # 4. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if image_paths:
            temp_dir = image_paths[0].parent
            print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_dir}")
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        # 5. æ‰“å°ç»Ÿè®¡
        print("\n" + "="*80)
        print("âœ… å¤„ç†å®Œæˆï¼")
        print("="*80)
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"  - æ€»é¡µæ•°: {total_pages}")
        print(f"  - æˆåŠŸ: {len([p for p in all_pages_data if 'error' not in p])}")
        print(f"  - å¤±è´¥: {len([p for p in all_pages_data if 'error' in p])}")
        print(f"  - è¾“å‡º: {output_path.absolute()}")
        
        return all_pages_data


def main():
    parser = argparse.ArgumentParser(description='PDF è½¬ç»“æ„åŒ– JSONï¼ˆä½¿ç”¨ LM Studioï¼‰')
    parser.add_argument('pdf_path', help='PDF æ–‡ä»¶è·¯å¾„')
    parser.add_argument('output_json', help='è¾“å‡º JSON æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    pdf_path = Path(args.pdf_path)
    if not pdf_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        sys.exit(1)
    
    if pdf_path.suffix.lower() != '.pdf':
        print(f"âŒ é”™è¯¯ï¼šä¸æ˜¯ PDF æ–‡ä»¶: {pdf_path}")
        sys.exit(1)
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = PDFToJSONProcessor()
        
        # å¤„ç† PDF
        processor.process_pdf(str(pdf_path), args.output_json)
        
        print("\nğŸ‰ å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()


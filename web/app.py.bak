"""FastAPI web application for RAG Knowledge Base"""

import os
import shutil
import json
import subprocess
import sys
import threading
from pathlib import Path
from typing import List, Optional
from datetime import datetime

import structlog
from fastapi import FastAPI, File, Form, HTTPException, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from starlette.requests import Request

from src.config import config
from src.pipeline import ProcessingPipeline
from src.database import DatabaseManager
from src.logging_config import setup_logging

# Initialize logging with file output
setup_logging(log_dir="logs", log_level="INFO")
logger = structlog.get_logger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="AIOps RAG Knowledge Base",
    description="AI-powered knowledge base for IT Operations and Security",
    version="1.1.0"
)

# CORS configuration
web_config = config.web_config
cors_config = web_config.get('cors', {})
if cors_config.get('enabled', True):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=cors_config.get('allow_origins', ["*"]),
        allow_credentials=True,
        allow_methods=cors_config.get('allow_methods', ["*"]),
        allow_headers=cors_config.get('allow_headers', ["*"]),
    )

# Setup templates and static files
templates = Jinja2Templates(directory="web/templates")
app.mount("/static", StaticFiles(directory="web/static"), name="static")

# Initialize pipeline and database
pipeline = ProcessingPipeline()
db = DatabaseManager()

# Create upload and processed folders
upload_folder = Path(web_config.get('upload_folder', './uploads'))
upload_folder.mkdir(parents=True, exist_ok=True)

processed_folder = Path('web/static/processed_docs')
processed_folder.mkdir(parents=True, exist_ok=True)


# Pydantic models
class SearchRequest(BaseModel):
    query: str
    k: int = 5
    filters: Optional[dict] = None
    use_hybrid: bool = True


class SearchResponse(BaseModel):
    results: List[dict]
    total: int


class MetadataUpdate(BaseModel):
    category: Optional[str] = None
    tags: Optional[List[str]] = None
    author: Optional[str] = None
    description: Optional[str] = None


# Routes
@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """Render main page"""
    return templates.TemplateResponse("index.html", {"request": request})


def process_document_background(doc_id: int, file_path: Path, metadata: dict, ocr_engine: str, checksum: str):
    """Background task to process document"""
    try:
        logger.info("background_processing_started", doc_id=doc_id, filename=file_path.name)
        
        # Update progress: Starting OCR
        db.update_document_progress(doc_id, 10, "Starting OCR processing...")
        
        # Run adaptive OCR pipeline
        adaptive_script = Path('document_ocr_pipeline/adaptive_ocr_pipeline.py')
        subprocess.run([
            sys.executable,
            str(adaptive_script),
            str(file_path),
            '--ocr-engine', ocr_engine
        ], check=True)
        
        # Update progress: OCR completed
        db.update_document_progress(doc_id, 50, "OCR completed, processing pages...")
        
        # Find the generated output directory
        temp_output_dir = Path(file_path.stem.replace(' ', '_') + "_adaptive")
        
        if not temp_output_dir.exists():
            raise RuntimeError(f"OCR output directory not found: {temp_output_dir}")
        
        # Move to static folder with doc ID
        doc_output_dir = processed_folder / f"{doc_id}_{checksum[:8]}"
        if doc_output_dir.exists():
            shutil.rmtree(doc_output_dir)
        shutil.move(str(temp_output_dir), str(doc_output_dir))
        
        # Update progress: Loading pages data
        db.update_document_progress(doc_id, 60, "Loading pages data...")
        
        # Load pages data
        complete_json = doc_output_dir / 'complete_adaptive_ocr.json'
        pages_data_list = []
        total_pages = 0
        
        if complete_json.exists():
            with open(complete_json, 'r', encoding='utf-8') as f:
                complete_data = json.load(f)
            
            total_pages = len(complete_data.get('pages', []))
            db.update_document_progress(doc_id, 65, f"Processing {total_pages} pages...", 
                                       processed_pages=0, total_pages=total_pages)
            
            # Build pages data
            for idx, page in enumerate(complete_data.get('pages', []), 1):
                page_num = page.get('page_number', idx)
                
                # Update progress per page
                page_progress = 65 + (20 * idx / total_pages)  # 65-85% for page processing
                db.update_document_progress(
                    doc_id, 
                    int(page_progress), 
                    f"Processing page {idx}/{total_pages}...",
                    processed_pages=idx,
                    total_pages=total_pages
                )
                
                # Get text count from statistics
                stats = page.get('statistics', {})
                text_count = stats.get('total_text_blocks', 0)
                
                # Get stage1 file paths
                stage1 = page.get('stage1_global', {})
                image_filename = stage1.get('image', f'page_{page_num:03d}_300dpi.png')
                visualized_filename = stage1.get('visualized', f'page_{page_num:03d}_global_visualized.png')
                ocr_json_filename = stage1.get('ocr_json', f'page_{page_num:03d}_global_ocr.json')
                
                # Try to extract components from VLM JSON if available
                components = []
                stage3 = page.get('stage3_vlm', {})
                vlm_json_filename = stage3.get('vlm_json')
                if vlm_json_filename:
                    vlm_json_path = doc_output_dir / vlm_json_filename
                    if vlm_json_path.exists():
                        try:
                            with open(vlm_json_path, 'r', encoding='utf-8') as vf:
                                vlm_data = json.load(vf)
                                # Try different possible locations for components
                                if 'components' in vlm_data:
                                    components = vlm_data['components']
                                elif 'domain_data' in vlm_data and isinstance(vlm_data['domain_data'], dict):
                                    if 'components' in vlm_data['domain_data']:
                                        components = vlm_data['domain_data']['components']
                                    elif 'equipment' in vlm_data['domain_data']:
                                        equipment = vlm_data['domain_data']['equipment']
                                        if isinstance(equipment, list):
                                            components = [e.get('id', '') for e in equipment if isinstance(e, dict) and 'id' in e]
                        except Exception as e:
                            logger.warning("failed_to_parse_vlm_json", error=str(e), file=vlm_json_filename)
                
                page_info = {
                    'page_num': page_num,
                    'image_path': f"/static/processed_docs/{doc_id}_{checksum[:8]}/{image_filename}",
                    'visualized_path': f"/static/processed_docs/{doc_id}_{checksum[:8]}/{visualized_filename}",
                    'ocr_json_path': f"/static/processed_docs/{doc_id}_{checksum[:8]}/{ocr_json_filename}",
                    'text_count': text_count,
                    'components': components[:20] if components else []
                }
                pages_data_list.append(page_info)
        
        # Update progress: Indexing to Elasticsearch
        db.update_document_progress(doc_id, 85, "Indexing to Elasticsearch...")
        
        # Process with vector store
        result = pipeline.process_file(str(file_path), metadata, processed_json_dir=str(doc_output_dir))
        
        # Update progress: Finalizing
        db.update_document_progress(doc_id, 95, "Finalizing...")
        
        # Update database with result
        if result.get('status') == 'completed':
            if not result.get('document_ids'):
                logger.error("NO_DOCUMENTS_INDEXED", 
                           num_chunks=result.get('num_chunks', 0), doc_id=doc_id)
                db.update_document_status(
                    doc_id,
                    'failed',
                    error_message='Processing completed but no documents were indexed to Elasticsearch'
                )
            else:
                db.update_document_status(
                    doc_id,
                    'completed',
                    num_chunks=result.get('num_chunks', 0),
                    es_document_ids=json.dumps(result.get('document_ids', [])),
                    pages_data=json.dumps(pages_data_list)
                )
                logger.info("document_processing_completed", doc_id=doc_id, 
                          num_chunks=result.get('num_chunks', 0))
        else:
            db.update_document_status(
                doc_id,
                'failed',
                error_message=result.get('error', 'Unknown error')
            )
    
    except subprocess.CalledProcessError as e:
        logger.error("adaptive_ocr_failed", error=str(e), doc_id=doc_id)
        db.update_document_status(
            doc_id,
            'failed',
            error_message=f"OCR processing failed: {str(e)}"
        )
    except (RuntimeError, ValueError) as e:
        logger.error("elasticsearch_operation_failed", error=str(e), 
                    error_type=type(e).__name__, doc_id=doc_id)
        db.update_document_status(
            doc_id,
            'failed',
            error_message=f"Elasticsearch operation failed: {str(e)}"
        )
    except Exception as e:
        logger.error("background_processing_failed", error=str(e), doc_id=doc_id)
        db.update_document_status(doc_id, 'failed', error_message=str(e))
    finally:
        # Clean up uploaded file
        if file_path and file_path.exists():
            try:
                os.remove(file_path)
                logger.info("cleaned_up_uploaded_file", file=str(file_path))
            except Exception as e:
                logger.warning("failed_to_cleanup_file", error=str(e), file=str(file_path))


@app.post("/upload")
async def upload_file(
    file: UploadFile = File(...),
    category: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),
    author: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    ocr_engine: Optional[str] = Form('easy')
):
    """
    Upload and process single file
    """
    doc_id = None
    file_path = None
    
    try:
        # Validate file
        if not file.filename:
            raise HTTPException(status_code=400, detail="No file provided")
        
        # Check file extension
        allowed_extensions = web_config.get('allowed_extensions', [])
        file_ext = Path(file.filename).suffix.lower().lstrip('.')
        
        if file_ext not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"File type not allowed. Allowed types: {', '.join(allowed_extensions)}"
            )
        
        # Save uploaded file
        file_path = upload_folder / file.filename
        with open(file_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        file_size = file_path.stat().st_size
        
        logger.info("file_uploaded", filename=file.filename, size=file_size)
        
        # Calculate checksum
        import hashlib
        with open(file_path, 'rb') as f:
            checksum = hashlib.sha256(f.read()).hexdigest()
        
        # Check if already exists
        existing = db.get_document_by_checksum(checksum)
        if existing:
            if file_path.exists():
                os.remove(file_path)
            return JSONResponse(content={
                "status": "duplicate",
                "message": "File already exists",
                "document": existing.to_dict()
            })
        
        # Create database record
        doc = db.create_document(
            filename=file.filename,
            file_path=str(file_path),
            file_type=file_ext,
            file_size=file_size,
            checksum=checksum,
            category=category,
            tags=tags.split(',') if tags else None,
            author=author,
            description=description,
            ocr_engine=ocr_engine
        )
        doc_id = doc.id
        
        # Update status to processing
        db.update_document_status(doc_id, 'processing')
        
        # Prepare metadata
        metadata = {}
        if category:
            metadata['category'] = category
        if tags:
            metadata['tags'] = tags.split(',')
        if author:
            metadata['author'] = author
        if description:
            metadata['description'] = description
        
        # Start background processing
        logger.info("starting_background_processing", doc_id=doc_id, filename=file.filename)
        
        # Start background thread
        thread = threading.Thread(
            target=process_document_background,
            args=(doc_id, file_path, metadata, ocr_engine, checksum),
            daemon=True
        )
        thread.start()
        
        # Return immediately with task info
        return JSONResponse(content={
            'status': 'processing',
            'message': 'Document uploaded and processing started',
            'document_id': doc_id,
            'checksum': checksum,
            'filename': file.filename
        })
    
    except Exception as e:
        logger.error("upload_failed", error=str(e))
        
        # Update database if record was created
        if doc_id:
            db.update_document_status(doc_id, 'failed', error_message=str(e))
        
        # Clean up file
        if file_path and file_path.exists():
            os.remove(file_path)
        
        raise HTTPException(status_code=500, detail=str(e))
            
            # Find the generated output directory
            temp_output_dir = Path(file_path.stem.replace(' ', '_') + "_adaptive")
            
            if not temp_output_dir.exists():
                raise RuntimeError(f"OCR output directory not found: {temp_output_dir}")
            
            # Move to static folder with doc ID
            doc_output_dir = processed_folder / f"{doc.id}_{checksum[:8]}"
            if doc_output_dir.exists():
                shutil.rmtree(doc_output_dir)
            shutil.move(str(temp_output_dir), str(doc_output_dir))
            
            # Load pages data
            complete_json = doc_output_dir / 'complete_adaptive_ocr.json'
            pages_data_list = []
            
            if complete_json.exists():
                with open(complete_json, 'r', encoding='utf-8') as f:
                    complete_data = json.load(f)
                
                # Build pages data
                for page in complete_data.get('pages', []):
                    page_num = page.get('page_number', 1)
                    
                    # Get text count from statistics
                    stats = page.get('statistics', {})
                    text_count = stats.get('total_text_blocks', 0)
                    
                    # Get stage1 file paths
                    stage1 = page.get('stage1_global', {})
                    image_filename = stage1.get('image', f'page_{page_num:03d}_300dpi.png')
                    visualized_filename = stage1.get('visualized', f'page_{page_num:03d}_global_visualized.png')
                    ocr_json_filename = stage1.get('ocr_json', f'page_{page_num:03d}_global_ocr.json')
                    
                    # Try to extract components from VLM JSON if available
                    components = []
                    stage3 = page.get('stage3_vlm', {})
                    vlm_json_filename = stage3.get('vlm_json')
                    if vlm_json_filename:
                        vlm_json_path = doc_output_dir / vlm_json_filename
                        if vlm_json_path.exists():
                            try:
                                with open(vlm_json_path, 'r', encoding='utf-8') as vf:
                                    vlm_data = json.load(vf)
                                    # Try different possible locations for components
                                    if 'components' in vlm_data:
                                        components = vlm_data['components']
                                    elif 'domain_data' in vlm_data and isinstance(vlm_data['domain_data'], dict):
                                        if 'components' in vlm_data['domain_data']:
                                            components = vlm_data['domain_data']['components']
                                        elif 'equipment' in vlm_data['domain_data']:
                                            equipment = vlm_data['domain_data']['equipment']
                                            if isinstance(equipment, list):
                                                components = [e.get('id', '') for e in equipment if isinstance(e, dict) and 'id' in e]
                            except Exception as e:
                                logger.warning("failed_to_parse_vlm_json", error=str(e), file=vlm_json_filename)
                    
                    page_info = {
                        'page_num': page_num,
                        'image_path': f"/static/processed_docs/{doc.id}_{checksum[:8]}/{image_filename}",
                        'visualized_path': f"/static/processed_docs/{doc.id}_{checksum[:8]}/{visualized_filename}",
                        'ocr_json_path': f"/static/processed_docs/{doc.id}_{checksum[:8]}/{ocr_json_filename}",
                        'text_count': text_count,
                        'components': components[:20] if components else []  # Limit to first 20
                    }
                    pages_data_list.append(page_info)
            
            # Process with vector store (pass processed JSON directory to avoid redundant VLM calls)
            result = pipeline.process_file(str(file_path), metadata, processed_json_dir=str(doc_output_dir))
            
            # Update database with result
            if result.get('status') == 'completed':
                # Additional validation: ensure documents were actually indexed
                if not result.get('document_ids'):
                    logger.error("‚ùå NO_DOCUMENTS_INDEXED", 
                               num_chunks=result.get('num_chunks', 0))
                    db.update_document_status(
                        doc_id,
                        'failed',
                        error_message='Processing completed but no documents were indexed to Elasticsearch'
                    )
                    result = {
                        'status': 'failed', 
                        'error': 'No documents indexed to Elasticsearch',
                        'document_id': doc_id,
                        'checksum': checksum
                    }
                else:
                    db.update_document_status(
                        doc_id,
                        'completed',
                        num_chunks=result.get('num_chunks', 0),
                        es_document_ids=json.dumps(result.get('document_ids', [])),
                        pages_data=json.dumps(pages_data_list)
                    )
            else:
                db.update_document_status(
                    doc_id,
                    'failed',
                    error_message=result.get('error', 'Unknown error')
                )
        except subprocess.CalledProcessError as e:
            logger.error("adaptive_ocr_failed", error=str(e))
            db.update_document_status(
                doc_id,
                'failed',
                error_message=f"OCR processing failed: {str(e)}"
            )
            result = {'status': 'failed', 'error': str(e)}
        except (RuntimeError, ValueError) as e:
            # Catch ES indexing failures (RuntimeError) and ES connection failures (ValueError)
            logger.error("elasticsearch_operation_failed", error=str(e), error_type=type(e).__name__)
            db.update_document_status(
                doc_id,
                'failed',
                error_message=f"Elasticsearch operation failed: {str(e)}"
            )
            result = {'status': 'failed', 'error': str(e)}
        
        # Clean up file
        if file_path and file_path.exists():
            os.remove(file_path)
        
        # Add document info to result if not already present
        if 'document_id' not in result:
            result['document_id'] = doc_id
        if 'checksum' not in result:
            result['checksum'] = checksum
        
        # Return appropriate HTTP status code
        if result.get('status') == 'failed':
            return JSONResponse(content=result, status_code=500)
        else:
            return JSONResponse(content=result)
    
    except Exception as e:
        logger.error("upload_failed", error=str(e))
        
        # Update database if record was created
        if doc_id:
            db.update_document_status(doc_id, 'failed', error_message=str(e))
        
        # Clean up file
        if file_path and file_path.exists():
            os.remove(file_path)
        
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/upload_batch")
async def upload_batch(
    files: List[UploadFile] = File(...),
    category: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),
    author: Optional[str] = Form(None)
):
    """
    Upload and process multiple files
    """
    try:
        file_paths = []
        
        # Save all files
        for file in files:
            file_path = upload_folder / file.filename
            with open(file_path, "wb") as f:
                shutil.copyfileobj(file.file, f)
            file_paths.append(str(file_path))
        
        logger.info("batch_uploaded", num_files=len(files))
        
        # Prepare metadata
        metadata = {}
        if category:
            metadata['category'] = category
        if tags:
            metadata['tags'] = tags.split(',')
        if author:
            metadata['author'] = author
        
        # Process batch
        results = pipeline.process_batch(file_paths, metadata)
        
        # Clean up
        for file_path in file_paths:
            if Path(file_path).exists():
                os.remove(file_path)
        
        return JSONResponse(content={"results": results})
    
    except Exception as e:
        logger.error("batch_upload_failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/upload_zip")
async def upload_zip(
    file: UploadFile = File(...),
    category: Optional[str] = Form(None),
    tags: Optional[str] = Form(None),
    author: Optional[str] = Form(None)
):
    """
    Upload and process ZIP file
    """
    try:
        if not file.filename.endswith('.zip'):
            raise HTTPException(status_code=400, detail="File must be a ZIP archive")
        
        # Save ZIP file
        zip_path = upload_folder / file.filename
        with open(zip_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        logger.info("zip_uploaded", filename=file.filename)
        
        # Prepare metadata
        metadata = {}
        if category:
            metadata['category'] = category
        if tags:
            metadata['tags'] = tags.split(',')
        if author:
            metadata['author'] = author
        
        # Process ZIP
        result = pipeline.process_zip(str(zip_path), metadata)
        
        # Clean up
        if zip_path.exists():
            os.remove(zip_path)
        
        # Clean up extracted files
        extract_dir = upload_folder / f"extracted_{zip_path.stem}"
        if extract_dir.exists():
            shutil.rmtree(extract_dir)
        
        return JSONResponse(content=result)
    
    except Exception as e:
        logger.error("zip_upload_failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/search", response_model=SearchResponse)
async def search(request: SearchRequest):
    """
    Search knowledge base
    """
    try:
        results = pipeline.search(
            query=request.query,
            k=request.k,
            filters=request.filters,
            use_hybrid=request.use_hybrid
        )
        
        # Enrich results with pages_data from database
        for result in results:
            metadata = result.get('metadata', {})
            checksum = metadata.get('checksum')
            
            if checksum:
                # Query database for document with this checksum
                doc = db.get_document_by_checksum(checksum)
                if doc and doc.pages_data:
                    try:
                        # Parse pages_data JSON and add to metadata
                        pages_data = json.loads(doc.pages_data) if isinstance(doc.pages_data, str) else doc.pages_data
                        metadata['pages_data'] = pages_data
                        metadata['ocr_engine'] = doc.ocr_engine
                    except json.JSONDecodeError:
                        logger.warning("failed_to_parse_pages_data", checksum=checksum)
        
        return SearchResponse(results=results, total=len(results))
    
    except Exception as e:
        logger.error("search_failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/component/{component_id}")
async def search_component(component_id: str, k: int = 10):
    """
    Search for pages containing specific component
    
    Args:
        component_id: Component ID (e.g., C1, V-2001, R100)
        k: Number of results to return
    
    Returns:
        List of pages containing the component
    """
    try:
        results = pipeline.search_component(
            component_id=component_id,
            k=k
        )
        
        return {
            "component_id": component_id,
            "results": results,
            "total": len(results)
        }
    
    except Exception as e:
        logger.error("component_search_failed", error=str(e), component_id=component_id)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/stats")
async def get_stats():
    """
    Get knowledge base statistics
    """
    try:
        # Get ES stats
        es_stats = pipeline.vector_store.get_stats()
        
        # Get database stats
        db_stats = db.get_stats()
        
        # Get ES index info
        es_client = pipeline.vector_store.es_client
        index_name = pipeline.vector_store.index_name
        
        # Check if index exists
        try:
            index_exists_response = es_client.indices.exists(index=index_name)
            # Handle both old and new ES client API responses
            if hasattr(index_exists_response, 'body'):
                index_exists = bool(index_exists_response.body)
            else:
                index_exists = bool(index_exists_response)
        except Exception:
            index_exists = False
        
        index_info = {
            'name': index_name,
            'exists': index_exists
        }
        
        if index_exists:
            index_info['status'] = 'green'  # Simplified, you can get real status from cluster health
            index_info['document_count'] = es_stats.get('document_count', 0)
        else:
            index_info['status'] = 'not_created'
            index_info['document_count'] = 0
        
        combined_stats = {
            **es_stats,
            'database': db_stats,
            'index': index_info
        }
        
        return JSONResponse(content=combined_stats)
    
    except Exception as e:
        logger.error("stats_retrieval_failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/documents")
async def delete_documents(filters: dict):
    """
    Delete documents by metadata filter
    """
    try:
        count = pipeline.vector_store.delete_by_metadata(filters)
        return JSONResponse(content={"deleted_count": count})
    
    except Exception as e:
        logger.error("deletion_failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/documents")
async def list_documents(limit: int = 50, offset: int = 0, status: Optional[str] = None):
    """List uploaded documents"""
    try:
        docs = db.list_documents(limit=limit, offset=offset, status=status)
        return JSONResponse(content={
            "documents": [doc.to_dict() for doc in docs],
            "total": len(docs)
        })
    except Exception as e:
        logger.error("list_documents_failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/documents/{doc_id}/progress")
async def get_document_progress(doc_id: int):
    """Get processing progress for a document"""
    try:
        doc = db.get_document(doc_id)
        if not doc:
            raise HTTPException(status_code=404, detail="Document not found")
        
        return JSONResponse(content={
            "doc_id": doc.id,
            "status": doc.status,
            "progress_percentage": doc.progress_percentage or 0,
            "progress_message": doc.progress_message or "",
            "total_pages": doc.total_pages or 0,
            "processed_pages": doc.processed_pages or 0,
            "filename": doc.filename
        })
    except HTTPException:
        raise
    except Exception as e:
        logger.error("get_progress_failed", error=str(e), doc_id=doc_id)
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/documents/{doc_id}")
async def delete_document(doc_id: int):
    """Delete a specific document by ID (both SQLite and ES)"""
    try:
        # 1. Get document info before deletion
        doc = db.get_document(doc_id)
        if not doc:
            raise HTTPException(status_code=404, detail="Document not found")
        
        checksum = doc.checksum
        
        # 2. Delete from SQLite
        success = db.delete_document(doc_id)
        if not success:
            raise HTTPException(status_code=404, detail="Failed to delete from database")
        
        # 3. Delete from Elasticsearch by checksum (document_id)
        try:
            es_deleted = pipeline.vector_store.delete_by_metadata({"document_id": checksum})
            logger.info("document_deleted", doc_id=doc_id, checksum=checksum, es_deleted=es_deleted)
        except Exception as es_error:
            logger.warning("es_deletion_failed", error=str(es_error), checksum=checksum)
            # Continue even if ES deletion fails
        
        return JSONResponse(content={
            "status": "success", 
            "message": f"Document {doc_id} deleted",
            "es_deleted_count": es_deleted if 'es_deleted' in locals() else 0
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("delete_document_failed", error=str(e), doc_id=doc_id)
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/documents")
async def delete_all_documents():
    """Delete all documents from both SQLite and Elasticsearch"""
    try:
        # 1. Get all document checksums before deletion
        all_docs = db.list_documents(limit=10000)
        checksums = [doc['checksum'] for doc in all_docs if doc.get('checksum')]
        
        # 2. Delete from SQLite
        db.delete_all_documents()
        
        # 3. Delete from Elasticsearch (all documents)
        es_deleted_total = 0
        for checksum in checksums:
            try:
                count = pipeline.vector_store.delete_by_metadata({"document_id": checksum})
                es_deleted_total += count
            except Exception as es_error:
                logger.warning("es_deletion_failed", error=str(es_error), checksum=checksum)
        
        logger.info("all_documents_deleted", sqlite_count=len(checksums), es_deleted=es_deleted_total)
        
        return JSONResponse(content={
            "status": "success", 
            "message": "All documents deleted",
            "sqlite_deleted": len(checksums),
            "es_deleted": es_deleted_total
        })
    except Exception as e:
        logger.error("delete_all_documents_failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy"}


if __name__ == "__main__":
    import uvicorn
    
    host = web_config.get('host', '0.0.0.0')
    port = web_config.get('port', 8000)
    
    logger.info("starting_web_server", host=host, port=port)
    
    uvicorn.run(app, host=host, port=port)

